{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T14:34:38.824433Z",
     "start_time": "2018-04-12T14:34:38.820469Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code taken from here : http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T14:34:39.058230Z",
     "start_time": "2018-04-12T14:34:38.826189Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from config import domainData\n",
    "from config import num_classes as NUM_CLASSES\n",
    "from wdStackDomain_alexnet_MADA import WDDomain\n",
    "from logger import Logger\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import logit\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "import datetime\n",
    "import itertools\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T14:34:39.255802Z",
     "start_time": "2018-04-12T14:34:39.059712Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "#         transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T14:34:39.516751Z",
     "start_time": "2018-04-12T14:34:39.257807Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_gpu = True and torch.cuda.is_available()\n",
    "train_dir = domainData['amazon'] # 'amazon', 'dslr', 'webcam'\n",
    "val_dir = domainData['webcam']\n",
    "num_classes = NUM_CLASSES['office']\n",
    "wd_param = 1.\n",
    "gp_lambda = 10\n",
    "num_iters = 50 # total number of training iterations\n",
    "base_lr = 1e-5\n",
    "EPOCHS = 50\n",
    "weight_decay = 1e-7\n",
    "g_loss_param = 1.\n",
    "batch_size = 32 # batch_size for each of source and target samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T14:34:39.716053Z",
     "start_time": "2018-04-12T14:34:39.518438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use gpu:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"use gpu: \", use_gpu)\n",
    "\n",
    "torch.manual_seed(7)\n",
    "if use_gpu:\n",
    "    torch.cuda.manual_seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T14:34:41.425972Z",
     "start_time": "2018-04-12T14:34:39.718221Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_datasets = {'train' : datasets.ImageFolder(train_dir,\n",
    "                                          data_transforms['train']),\n",
    "                  'val' : datasets.ImageFolder(val_dir,\n",
    "                                          data_transforms['val'])\n",
    "                 }\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=4, drop_last=True)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "model_ft = WDDomain(num_classes)\n",
    "\n",
    "if use_gpu:\n",
    "    model_ft = model_ft.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T14:34:41.432307Z",
     "start_time": "2018-04-12T14:34:41.427795Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inifinite_dataloader(dataloader):\n",
    "    data_iter = iter(dataloader)\n",
    "    while(1):\n",
    "        try:\n",
    "            data = next(data_iter)\n",
    "            yield data\n",
    "        except StopIteration:\n",
    "            data_iter = iter(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T04:23:41.472664Z",
     "start_time": "2018-04-12T04:23:41.469696Z"
    }
   },
   "source": [
    "# Using broadcasting for creating predicted label feature tensor\n",
    "\n",
    "```python\n",
    "a = torch.Tensor([[1,2,3,4,5],[10,20,30,40,50]])\n",
    "b = torch.ones(2,14)\n",
    "\n",
    "a.unsqueeze_(-1)\n",
    "b.unsqueeze_(1)\n",
    "print(a.size(), b.size())\n",
    "\n",
    "print(a * b)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T14:34:41.657136Z",
     "start_time": "2018-04-12T14:34:41.433563Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clscriterion = nn.CrossEntropyLoss()\n",
    "# clscriterion = logit.softmax_cross_entropy_with_logits(num_classes)\n",
    "\n",
    "param_group1 = [\n",
    "# {'params' : model_ft.features.parameters(), 'lr' : 1e-4, 'betas' : (0.5, 0.9)},\n",
    "{'params' : model_ft._discriminator.parameters(), 'lr': 10*base_lr, 'weight_decay' : weight_decay } # , 'lr' : 1e-4, 'betas' : (0.5, 0.9)\n",
    "]\n",
    "disc_opt = optim.Adam(param_group1)\n",
    "# disc_opt = optim.SGD(param_group1, momentum=0.9)\n",
    "\n",
    "param_group2 = [\n",
    "{'params' : model_ft.features.net.classifier.parameters(), 'lr' : 0.001*base_lr, 'weight_decay' : weight_decay}, # , 'betas' : (0.5, 0.9\n",
    "{'params' : model_ft.features.extra.parameters(), 'lr' : 10*base_lr, 'weight_decay' : weight_decay},\n",
    "{'params' : model_ft.classifier.parameters(), 'lr': 10*base_lr, 'weight_decay' : weight_decay}\n",
    "]\n",
    "gen_opt = optim.Adam(param_group2)\n",
    "# gen_opt = optim.SGD(param_group2, momentum=0.9)\n",
    "\n",
    "# param_group3 = [\n",
    "# # {'params' : model_ft.features.basenet.layer4.parameters(), 'lr' : 2e-4, 'betas' : (0.5, 0.9), 'weight_decay' : 1e-4},\n",
    "# # {'params' : model_ft.features.net.classifier.parameters(), 'lr': base_lr, 'weight_decay' : 1e-5}, # , 'lr' : 1e-4, 'betas' : (0.5, 0.9)\n",
    "# {'params' : model_ft.features.extra.parameters(), 'lr': 10*base_lr, 'weight_decay' : 1e-5},\n",
    "# {'params' : model_ft.classifier.parameters(), 'lr': 10*base_lr, 'weight_decay' : 1e-4} # , 'lr' : 1e-4\n",
    "# ]\n",
    "cls_opt = None\n",
    "# cls_opt = optim.Adam(param_group3)\n",
    "# cls_opt = optim.SGD(param_group3, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "# tar_lr_scheduler = lr_scheduler.StepLR(taroptimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# cls_scheduler = lr_scheduler.ExponentialLR(cls_opt, 0.1)\n",
    "# gen_scheduler = lr_scheduler.ExponentialLR(gen_opt, 0.1)\n",
    "# disc_scheduler = lr_scheduler.ExponentialLR(disc_opt, 0.1)\n",
    "\n",
    "disc_sch = lr_scheduler.StepLR(disc_opt, 10, 0.1)\n",
    "gen_sch = lr_scheduler.StepLR(gen_opt, 10, 0.1)\n",
    "\n",
    "# lr_schedulers = [cls_scheduler, gen_scheduler, disc_scheduler]\n",
    "lr_schedulers = [disc_sch, gen_sch]\n",
    "# lr_schedulers = None\n",
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "l2_reg = logit.L2_Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T14:34:41.827939Z",
     "start_time": "2018-04-12T14:34:41.658701Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(model_ft, criterion, save_model=False, save_name=None):\n",
    "    data_iter = iter(dataloaders['val'])\n",
    "\n",
    "    model_ft.features.eval()\n",
    "    model_ft.classifier.eval()\n",
    "    model_ft._discriminator.eval()\n",
    "    \n",
    "    acc_val = 0\n",
    "    steps = 0.\n",
    "    for data in data_iter:\n",
    "        img, lbl = data\n",
    "        if use_gpu:\n",
    "            img = img.cuda()\n",
    "            lbl = lbl.cuda()\n",
    "        img = Variable(img, volatile=True)\n",
    "        lbl = Variable(lbl, requires_grad=False)\n",
    "\n",
    "        feat_out = model_ft.features(img)\n",
    "        out = model_ft.classifier(feat_out)\n",
    "\n",
    "        loss = criterion(out, lbl)\n",
    "        \n",
    "        out1 = softmax(out)\n",
    "        _, preds = torch.max(out1.data, 1)\n",
    "#         print(\"preds size: \", preds.size(), \" lbl data size: \", lbl.data.size())\n",
    "        acc_val += torch.eq(preds, lbl.data).float().mean()\n",
    "        steps = steps + 1\n",
    "    # acc = acc_val / dataset_sizes['val']\n",
    "    acc = acc_val / steps\n",
    "    print(\"validation accuracy: {:.4f}\".format(acc))\n",
    "    if save_model:\n",
    "        torch.save(model_ft.state_dict(), save_name)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T14:34:42.202428Z",
     "start_time": "2018-04-12T14:34:41.829625Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, clscriterion, disc_opt, gen_opt, cls_opt=None, lr_schedulers=None, num_epochs=25):\n",
    "    since = time.time()\n",
    "   \n",
    "    for _ in range(num_iters):\n",
    "        src_data = iter(dataloaders['train'])\n",
    "        tar_data = get_inifinite_dataloader(dataloaders['val'])\n",
    "        running_clsacc, running_clsloss = 0., 0.\n",
    "        running_critic_loss, running_gen_loss, running_gp_loss, steps = 0., 0., 0., 0\n",
    "\n",
    "        for srcinps, srclbls in src_data:\n",
    "            tarinps, _ = next(tar_data)\n",
    "\n",
    "            srcinps = srcinps.cuda() if use_gpu else srcinps\n",
    "            srclbls = srclbls.cuda() if use_gpu else srclbls\n",
    "            tarinps = tarinps.cuda() if use_gpu else tarinps\n",
    "            srcinps, srclbls = Variable(srcinps), Variable(srclbls)\n",
    "            tarinps = Variable(tarinps)\n",
    "\n",
    "            inps = torch.cat([srcinps, tarinps])\n",
    "            feats = model.features(inps) # feats size: [batch_size, 256]\n",
    "\n",
    "            cls_out = model.classifier(feats)\n",
    "            cls_out_src = cls_out[:batch_size]\n",
    "            cls_out_tar = cls_out[batch_size:]\n",
    "\n",
    "            cls_loss = clscriterion(cls_out_src, srclbls)\n",
    "            _, preds = torch.max(softmax(cls_out_src).data, 1)\n",
    "            running_clsacc += torch.eq(preds, srclbls.data).float().mean()\n",
    "            running_clsloss += cls_loss.data[0]\n",
    "\n",
    "            cls_out_tar = softmax(cls_out_tar)\n",
    "            assert cls_out_tar.size(1)==num_classes, \"expected %d but got %s\"%(num_classes, str(cls_out_tar.size()))\n",
    "\n",
    "            srclbls_onehot = utils.one_hot_encode(srclbls, num_classes)\n",
    "            assert srclbls_onehot.size()==cls_out_tar.size(), \"expected same, got %s %s\"%(str(srclbls_onehot.size(), cls_out_tar.size()))\n",
    "\n",
    "            mode_wts = torch.cat([srclbls_onehot, cls_out_tar.data]) # size: [batch, num_classes]\n",
    "            mode_wts.unsqueeze_(-1) # size: [batch, num_classes, 1]\n",
    "            mode_wts = Variable(mode_wts, requires_grad=False)\n",
    "            \n",
    "            critic_in = feats.unsqueeze(1) # size: [batch, 1, 256]            \n",
    "            critic_in = critic_in * mode_wts # critic_in size: [batch_size, num_classes, 256]\n",
    "\n",
    "            critic_out = model._discriminator(critic_in)\n",
    "            D = critic_out[:batch_size]\n",
    "            D_ = critic_out[batch_size:]\n",
    "            critic_loss = (torch.sigmoid(D_) ** 2).mean() + ((1. - torch.sigmoid(D)) ** 2).mean()\n",
    "            gen_loss = D.mean() - D_.mean()\n",
    "\n",
    "            running_critic_loss += critic_loss.data[0]\n",
    "            running_gen_loss += gen_loss.data[0]\n",
    "\n",
    "            feats_wt_src = critic_in[:batch_size].data\n",
    "            feats_wt_tar = critic_in[batch_size:].data\n",
    "\n",
    "            alpha = torch.Tensor(batch_size, 1).uniform_(0,1).unsqueeze_(-1)\n",
    "            alpha = alpha.cuda() if use_gpu else alpha\n",
    "            diff = feats_wt_src - feats_wt_tar\n",
    "            interpolates = feats_wt_src + (alpha * diff)\n",
    "            interpolates = Variable(interpolates, requires_grad=True)\n",
    "\n",
    "            inter_out = model._discriminator(interpolates)\n",
    "            ones = torch.ones(inter_out.size())\n",
    "            ones = ones.cuda() if use_gpu else ones\n",
    "\n",
    "            grads = autograd.grad(inter_out, interpolates, grad_outputs=ones,\n",
    "                retain_graph=True, create_graph=True, only_inputs=False)[0]\n",
    "            gp = ((grads.norm(2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "            running_gp_loss += gp.data[0]\n",
    "\n",
    "            for params in model._discriminator.parameters():\n",
    "                params.requires_grad=True\n",
    "\n",
    "            critic_loss_t = critic_loss * wd_param + gp * gp_lambda\n",
    "            disc_opt.zero_grad()\n",
    "            critic_loss_t.backward(retain_graph=True)\n",
    "            disc_opt.step()\n",
    "\n",
    "            for params in model._discriminator.parameters():\n",
    "                params.requires_grad=False\n",
    "\n",
    "            total_loss = cls_loss + gen_loss * g_loss_param\n",
    "\n",
    "            gen_opt.zero_grad()\n",
    "            total_loss.backward()\n",
    "            gen_opt.step()\n",
    "            steps = steps + 1\n",
    "            \n",
    "        print(\"critic loss: {:.4f}, gen loss: {:.4f}, gp loss: {:.4f}\".\n",
    "              format(running_critic_loss/steps, running_gen_loss/steps, running_gp_loss/steps))\n",
    "        print(\"classification loss: {:.4f}, classification accuracy: {:.4f}\".\n",
    "             format(running_clsloss/steps, running_clsacc/steps))\n",
    "\n",
    "        if lr_schedulers:\n",
    "            for x in lr_schedulers:\n",
    "                x.step()\n",
    "\n",
    "        test_model(model, clscriterion, False, None)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T05:53:40.833790Z",
     "start_time": "2018-04-12T05:53:40.827672Z"
    }
   },
   "source": [
    "```python\n",
    "a = torch.Tensor(32,31,256).uniform_(0,1)\n",
    "\n",
    "conv1 = nn.Sequential(nn.Conv1d(31,31,70), nn.Conv1d(31,31,130), nn.Conv1d(31,16,58), nn.Conv1d(16,1,1))\n",
    "\n",
    "b = conv1(Variable(a))\n",
    "\n",
    "b.size()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-12T14:51:43.493392Z",
     "start_time": "2018-04-12T14:34:42.203987Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "critic loss: 0.5000, gen loss: 0.0075, gp loss: 0.9971\n",
      "classification loss: 1.7858, classification accuracy: 0.5781\n",
      "validation accuracy: 0.4779\n",
      "critic loss: 0.5004, gen loss: 0.0063, gp loss: 0.9971\n",
      "classification loss: 0.8669, classification accuracy: 0.7887\n",
      "validation accuracy: 0.5000\n",
      "critic loss: 0.5009, gen loss: 0.0044, gp loss: 0.9971\n",
      "classification loss: 0.6483, classification accuracy: 0.8391\n",
      "validation accuracy: 0.4961\n",
      "critic loss: 0.5013, gen loss: 0.0029, gp loss: 0.9970\n",
      "classification loss: 0.5147, classification accuracy: 0.8764\n",
      "validation accuracy: 0.5091\n",
      "critic loss: 0.5017, gen loss: 0.0013, gp loss: 0.9970\n",
      "classification loss: 0.4181, classification accuracy: 0.8956\n",
      "validation accuracy: 0.4909\n",
      "critic loss: 0.5022, gen loss: -0.0004, gp loss: 0.9970\n",
      "classification loss: 0.3373, classification accuracy: 0.9293\n",
      "validation accuracy: 0.5039\n",
      "critic loss: 0.5026, gen loss: -0.0022, gp loss: 0.9969\n",
      "classification loss: 0.2768, classification accuracy: 0.9457\n",
      "validation accuracy: 0.4974\n",
      "critic loss: 0.5031, gen loss: -0.0040, gp loss: 0.9969\n",
      "classification loss: 0.2337, classification accuracy: 0.9553\n",
      "validation accuracy: 0.4896\n",
      "critic loss: 0.5036, gen loss: -0.0061, gp loss: 0.9969\n",
      "classification loss: 0.1911, classification accuracy: 0.9666\n",
      "validation accuracy: 0.4831\n",
      "critic loss: 0.5042, gen loss: -0.0084, gp loss: 0.9969\n",
      "classification loss: 0.1571, classification accuracy: 0.9762\n",
      "validation accuracy: 0.4857\n",
      "critic loss: 0.5048, gen loss: -0.0108, gp loss: 0.9969\n",
      "classification loss: 0.1326, classification accuracy: 0.9844\n",
      "validation accuracy: 0.4740\n",
      "critic loss: 0.5052, gen loss: -0.0124, gp loss: 0.9969\n",
      "classification loss: 0.1082, classification accuracy: 0.9897\n",
      "validation accuracy: 0.4766\n",
      "critic loss: 0.5053, gen loss: -0.0127, gp loss: 0.9969\n",
      "classification loss: 0.1051, classification accuracy: 0.9904\n",
      "validation accuracy: 0.4714\n",
      "critic loss: 0.5054, gen loss: -0.0130, gp loss: 0.9969\n",
      "classification loss: 0.1040, classification accuracy: 0.9911\n",
      "validation accuracy: 0.4701\n",
      "critic loss: 0.5054, gen loss: -0.0132, gp loss: 0.9969\n",
      "classification loss: 0.1000, classification accuracy: 0.9929\n",
      "validation accuracy: 0.4753\n",
      "critic loss: 0.5055, gen loss: -0.0137, gp loss: 0.9969\n",
      "classification loss: 0.0990, classification accuracy: 0.9940\n",
      "validation accuracy: 0.4688\n",
      "critic loss: 0.5056, gen loss: -0.0140, gp loss: 0.9969\n",
      "classification loss: 0.0966, classification accuracy: 0.9929\n",
      "validation accuracy: 0.4714\n",
      "critic loss: 0.5057, gen loss: -0.0144, gp loss: 0.9969\n",
      "classification loss: 0.0951, classification accuracy: 0.9933\n",
      "validation accuracy: 0.4661\n",
      "critic loss: 0.5058, gen loss: -0.0147, gp loss: 0.9968\n",
      "classification loss: 0.0920, classification accuracy: 0.9929\n",
      "validation accuracy: 0.4701\n",
      "critic loss: 0.5059, gen loss: -0.0151, gp loss: 0.9968\n",
      "classification loss: 0.0902, classification accuracy: 0.9933\n",
      "validation accuracy: 0.4688\n",
      "critic loss: 0.5060, gen loss: -0.0154, gp loss: 0.9968\n",
      "classification loss: 0.0888, classification accuracy: 0.9947\n",
      "validation accuracy: 0.4766\n",
      "critic loss: 0.5061, gen loss: -0.0160, gp loss: 0.9969\n",
      "classification loss: 0.0866, classification accuracy: 0.9936\n",
      "validation accuracy: 0.4661\n",
      "critic loss: 0.5061, gen loss: -0.0158, gp loss: 0.9969\n",
      "classification loss: 0.0878, classification accuracy: 0.9943\n",
      "validation accuracy: 0.4753\n",
      "critic loss: 0.5061, gen loss: -0.0159, gp loss: 0.9969\n",
      "classification loss: 0.0870, classification accuracy: 0.9940\n",
      "validation accuracy: 0.4674\n",
      "critic loss: 0.5061, gen loss: -0.0159, gp loss: 0.9969\n",
      "classification loss: 0.0856, classification accuracy: 0.9947\n",
      "validation accuracy: 0.4727\n",
      "critic loss: 0.5061, gen loss: -0.0160, gp loss: 0.9968\n",
      "classification loss: 0.0854, classification accuracy: 0.9950\n",
      "validation accuracy: 0.4661\n",
      "critic loss: 0.5061, gen loss: -0.0161, gp loss: 0.9968\n",
      "classification loss: 0.0855, classification accuracy: 0.9943\n",
      "validation accuracy: 0.4661\n",
      "critic loss: 0.5062, gen loss: -0.0162, gp loss: 0.9968\n",
      "classification loss: 0.0858, classification accuracy: 0.9943\n",
      "validation accuracy: 0.4661\n",
      "critic loss: 0.5061, gen loss: -0.0161, gp loss: 0.9968\n",
      "classification loss: 0.0852, classification accuracy: 0.9950\n",
      "validation accuracy: 0.4622\n",
      "critic loss: 0.5062, gen loss: -0.0162, gp loss: 0.9968\n",
      "classification loss: 0.0851, classification accuracy: 0.9947\n",
      "validation accuracy: 0.4701\n",
      "critic loss: 0.5062, gen loss: -0.0162, gp loss: 0.9969\n",
      "classification loss: 0.0848, classification accuracy: 0.9950\n",
      "validation accuracy: 0.4648\n",
      "critic loss: 0.5062, gen loss: -0.0163, gp loss: 0.9968\n",
      "classification loss: 0.0850, classification accuracy: 0.9943\n",
      "validation accuracy: 0.4661\n",
      "critic loss: 0.5062, gen loss: -0.0163, gp loss: 0.9969\n",
      "classification loss: 0.0859, classification accuracy: 0.9950\n",
      "validation accuracy: 0.4714\n",
      "critic loss: 0.5062, gen loss: -0.0163, gp loss: 0.9969\n",
      "classification loss: 0.0830, classification accuracy: 0.9961\n",
      "validation accuracy: 0.4688\n",
      "critic loss: 0.5062, gen loss: -0.0164, gp loss: 0.9969\n",
      "classification loss: 0.0833, classification accuracy: 0.9947\n",
      "validation accuracy: 0.4661\n",
      "critic loss: 0.5062, gen loss: -0.0163, gp loss: 0.9969\n",
      "classification loss: 0.0857, classification accuracy: 0.9947\n",
      "validation accuracy: 0.4648\n",
      "critic loss: 0.5062, gen loss: -0.0163, gp loss: 0.9968\n",
      "classification loss: 0.0833, classification accuracy: 0.9954\n",
      "validation accuracy: 0.4727\n",
      "critic loss: 0.5062, gen loss: -0.0163, gp loss: 0.9968\n",
      "classification loss: 0.0836, classification accuracy: 0.9957\n",
      "validation accuracy: 0.4635\n",
      "critic loss: 0.5062, gen loss: -0.0163, gp loss: 0.9969\n",
      "classification loss: 0.0830, classification accuracy: 0.9947\n",
      "validation accuracy: 0.4622\n",
      "critic loss: 0.5062, gen loss: -0.0164, gp loss: 0.9969\n",
      "classification loss: 0.0838, classification accuracy: 0.9943\n",
      "validation accuracy: 0.4688\n",
      "critic loss: 0.5062, gen loss: -0.0164, gp loss: 0.9968\n",
      "classification loss: 0.0846, classification accuracy: 0.9950\n",
      "validation accuracy: 0.4688\n",
      "critic loss: 0.5062, gen loss: -0.0164, gp loss: 0.9968\n",
      "classification loss: 0.0839, classification accuracy: 0.9943\n",
      "validation accuracy: 0.4688\n",
      "critic loss: 0.5062, gen loss: -0.0164, gp loss: 0.9968\n",
      "classification loss: 0.0835, classification accuracy: 0.9950\n",
      "validation accuracy: 0.4714\n",
      "critic loss: 0.5062, gen loss: -0.0163, gp loss: 0.9968\n",
      "classification loss: 0.0858, classification accuracy: 0.9950\n",
      "validation accuracy: 0.4661\n",
      "critic loss: 0.5062, gen loss: -0.0164, gp loss: 0.9968\n",
      "classification loss: 0.0848, classification accuracy: 0.9940\n",
      "validation accuracy: 0.4648\n",
      "critic loss: 0.5062, gen loss: -0.0164, gp loss: 0.9968\n",
      "classification loss: 0.0827, classification accuracy: 0.9943\n",
      "validation accuracy: 0.4688\n",
      "critic loss: 0.5062, gen loss: -0.0164, gp loss: 0.9968\n",
      "classification loss: 0.0827, classification accuracy: 0.9954\n",
      "validation accuracy: 0.4688\n",
      "critic loss: 0.5062, gen loss: -0.0162, gp loss: 0.9969\n",
      "classification loss: 0.0843, classification accuracy: 0.9954\n",
      "validation accuracy: 0.4674\n",
      "critic loss: 0.5062, gen loss: -0.0164, gp loss: 0.9968\n",
      "classification loss: 0.0831, classification accuracy: 0.9950\n",
      "validation accuracy: 0.4674\n",
      "critic loss: 0.5062, gen loss: -0.0165, gp loss: 0.9968\n",
      "classification loss: 0.0840, classification accuracy: 0.9943\n",
      "validation accuracy: 0.4648\n",
      "Training complete in 16m 53s\n",
      "validation accuracy: 0.4674\n",
      "validation accuracy: 0.4661\n",
      "validation accuracy: 0.4688\n",
      "validation accuracy: 0.4701\n",
      "validation accuracy: 0.4674\n"
     ]
    }
   ],
   "source": [
    "train_model(model_ft, clscriterion, disc_opt, gen_opt, cls_opt, lr_schedulers, num_epochs=EPOCHS)\n",
    "\n",
    "save_name = \"grl_model_with_transform.pth\"\n",
    "test_model(model_ft, clscriterion, False, save_name)\n",
    "test_model(model_ft, clscriterion, False, save_name)\n",
    "test_model(model_ft, clscriterion, False, save_name)\n",
    "test_model(model_ft, clscriterion, False, save_name)\n",
    "test_model(model_ft, clscriterion, False, save_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
