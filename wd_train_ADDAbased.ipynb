{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T11:07:11.255993Z",
     "start_time": "2018-04-13T11:07:11.249670Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code taken from here : http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T11:07:11.598880Z",
     "start_time": "2018-04-13T11:07:11.260173Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from config import domainData\n",
    "from config import num_classes as NUM_CLASSES\n",
    "from wdStackDomain_alexnet import WDDomain\n",
    "from logger import Logger\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import logit\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "import datetime\n",
    "import itertools\n",
    "from tqdm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T11:07:11.764028Z",
     "start_time": "2018-04-13T11:07:11.600656Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "#         transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T11:07:12.012965Z",
     "start_time": "2018-04-13T11:07:11.765562Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "use_gpu = True and torch.cuda.is_available()\n",
    "train_dir = domainData['amazon'] # 'amazon', 'dslr', 'webcam'\n",
    "val_dir = domainData['webcam']\n",
    "num_classes = NUM_CLASSES['office']\n",
    "wd_param = 0.1\n",
    "gp_lambda = 10\n",
    "num_iters = 75 # total number of training iterations\n",
    "num_cls_train = 2 # epochs to train classifier\n",
    "num_gen_train = 1 # epochs to train generator\n",
    "base_lr = 1e-5\n",
    "l2_param = 1e-5\n",
    "weight_decay = 1e-6\n",
    "g_loss_param = 0.2\n",
    "EPOCHS=50\n",
    "batch_size = 64 # batch_size for each of source and target samples\n",
    "load_cls = False\n",
    "log = False\n",
    "text_log = True\n",
    "exp_name = 'wd_tr_2step_Rsnt2blk_r3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T11:07:12.186906Z",
     "start_time": "2018-04-13T11:07:12.014386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use gpu:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"use gpu: \", use_gpu)\n",
    "\n",
    "torch.manual_seed(7)\n",
    "if use_gpu:\n",
    "    torch.cuda.manual_seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T11:07:20.275718Z",
     "start_time": "2018-04-13T11:07:12.188490Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_datasets = {'train' : datasets.ImageFolder(train_dir,\n",
    "                                          data_transforms['train']),\n",
    "                  'val' : datasets.ImageFolder(val_dir,\n",
    "                                          data_transforms['val'])\n",
    "                 }\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=4, drop_last=True)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "model_ft = WDDomain(num_classes)\n",
    "\n",
    "if load_cls:\n",
    "    model_ft.load_cls()\n",
    "\n",
    "if use_gpu:\n",
    "    model_ft = model_ft.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T11:07:20.281449Z",
     "start_time": "2018-04-13T11:07:20.277087Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inifinite_dataloader(dataloader):\n",
    "    data_iter = iter(dataloader)\n",
    "    while(1):\n",
    "        try:\n",
    "            data = next(data_iter)\n",
    "            yield data\n",
    "        except StopIteration:\n",
    "            data_iter = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T11:07:20.546706Z",
     "start_time": "2018-04-13T11:07:20.282667Z"
    }
   },
   "outputs": [],
   "source": [
    "# clscriterion = nn.CrossEntropyLoss()\n",
    "clscriterion = logit.softmax_cross_entropy_with_logits(num_classes)\n",
    "\n",
    "param_group1 = [\n",
    "{'params' : model_ft._discriminator.parameters(), 'lr': 10*base_lr, 'weight_decay' : weight_decay } # , 'lr' : 1e-4, 'betas' : (0.5, 0.9)\n",
    "]\n",
    "# disc_opt = optim.Adam(param_group1)\n",
    "# disc_opt = optim.ASGD(param_group1)\n",
    "disc_opt = optim.SGD(param_group1, momentum=0.9)\n",
    "\n",
    "param_group2 = [\n",
    "{'params' : model_ft.features.net.features.parameters(), 'lr' : 0.01 * base_lr,\n",
    " 'weight_decay' : weight_decay},\n",
    "{'params' : model_ft.features.net.classifier.parameters(), 'lr' : base_lr, 'weight_decay' : weight_decay}, # , 'betas' : (0.5, 0.9\n",
    "{'params' : model_ft.features.extra.parameters(), 'lr' : 10*base_lr, 'weight_decay' : weight_decay}\n",
    "]\n",
    "# gen_opt = optim.Adam(param_group2)\n",
    "# gen_opt = optim.ASGD(param_group2)\n",
    "gen_opt = optim.SGD(param_group2, momentum=0.9)\n",
    "\n",
    "param_group3 = [\n",
    "{'params' : model_ft.features.net.features.parameters(), 'lr' : 0.01 * base_lr,\n",
    " 'weight_decay' : weight_decay},\n",
    "{'params' : model_ft.features.net.classifier.parameters(), 'lr' : base_lr, 'weight_decay' : weight_decay}, # , 'betas' : (0.5, 0.9\n",
    "{'params' : model_ft.features.extra.parameters(), 'lr' : 10*base_lr, 'weight_decay' : weight_decay},\n",
    "{'params' : model_ft.classifier.parameters(), 'lr': 10*base_lr, 'weight_decay' : weight_decay} # , 'lr' : 1e-4\n",
    "]\n",
    "# cls_opt = optim.Adam(param_group3)\n",
    "# cls_opt = optim.ASGD(param_group3)\n",
    "cls_opt = optim.SGD(param_group3, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "# tar_lr_scheduler = lr_scheduler.StepLR(taroptimizer, step_size=7, gamma=0.1)\n",
    "\n",
    "# cls_scheduler = None\n",
    "cls_scheduler = lr_scheduler.StepLR(cls_opt, 20, 0.1)\n",
    "# cls_scheduler = lr_scheduler.ExponentialLR(cls_opt, 0.1)\n",
    "# gen_scheduler = None\n",
    "gen_scheduler = lr_scheduler.StepLR(gen_opt, 20, 0.1)\n",
    "# gen_scheduler = lr_scheduler.ExponentialLR(gen_opt, 0.1)\n",
    "# disc_scheduler = None\n",
    "disc_scheduler = lr_scheduler.StepLR(disc_opt, 20, 0.1)\n",
    "# disc_scheduler = lr_scheduler.ExponentialLR(disc_opt, 0.1)\n",
    "\n",
    "lr_schedulers = [cls_scheduler, gen_scheduler, disc_scheduler]\n",
    "# lr_schedulers = None\n",
    "\n",
    "softmax = nn.Softmax(dim=1)\n",
    "l2_reg = logit.L2_Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T11:07:20.859260Z",
     "start_time": "2018-04-13T11:07:20.548460Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_model(model_ft, criterion, save_model=False, save_name=None):\n",
    "    data_iter = iter(dataloaders['val'])\n",
    "\n",
    "    model_ft.features.eval()\n",
    "    model_ft.classifier.eval()\n",
    "    model_ft._discriminator.eval()\n",
    "    \n",
    "    acc_val = 0\n",
    "    steps = 0.\n",
    "    for data in data_iter:\n",
    "        img, lbl = data\n",
    "        if use_gpu:\n",
    "            img = img.cuda()\n",
    "            lbl = lbl.cuda()\n",
    "        img = Variable(img, volatile=True)\n",
    "        lbl = Variable(lbl, requires_grad=False)\n",
    "\n",
    "        feat_out = model_ft.features(img)\n",
    "        out = model_ft.classifier(feat_out)\n",
    "\n",
    "        loss = criterion(out, lbl)\n",
    "        \n",
    "        out1 = softmax(out)\n",
    "        _, preds = torch.max(out1.data, 1)\n",
    "#         print(\"preds size: \", preds.size(), \" lbl data size: \", lbl.data.size())\n",
    "        acc_val += torch.eq(preds, lbl.data).float().mean()\n",
    "        steps = steps + 1\n",
    "    # acc = acc_val / dataset_sizes['val']\n",
    "    acc = acc_val / steps\n",
    "    print(\"validation accuracy: {:.4f}\".format(acc))\n",
    "    if text_log:\n",
    "        text_file.write(\"validation accuracy: {:.4f}\\n\".format(acc))\n",
    "    if save_model:\n",
    "        torch.save(model_ft.state_dict(), save_name)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T11:07:21.553239Z",
     "start_time": "2018-04-13T11:07:20.860651Z"
    },
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, clscriterion, disc_opt, gen_opt, cls_opt, lr_schedulers=None, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        \n",
    "        running_clsacc, running_clsloss, steps = 0., 0., 0\n",
    "        \n",
    "        # train classifier on source\n",
    "        for _ in range(num_cls_train):\n",
    "            cls_src = iter(dataloaders['train'])\n",
    "            for data in cls_src:\n",
    "                srcimgs, srclbls = data\n",
    "                \n",
    "                srcimgs = srcimgs.cuda() if use_gpu else srcimgs\n",
    "                srclbls = srclbls.cuda() if use_gpu else srclbls\n",
    "                srcimgs, srclbls = Variable(srcimgs), Variable(srclbls)\n",
    "                \n",
    "                outimgs = model.features(srcimgs)\n",
    "                cls_out = model.classifier(outimgs)\n",
    "                cls_loss = clscriterion(cls_out, srclbls)\n",
    "                \n",
    "#                 l2_loss = None\n",
    "#                 for name, param in model.classifier.named_parameters():\n",
    "#                     if 'weight' in name:\n",
    "#                         if l2_loss is None:\n",
    "#                             l2_loss = l2_reg(param) * l2_param\n",
    "#                         else:\n",
    "#                             l2_loss += l2_reg(param) * l2_param\n",
    "#                 for name, param in model.features.named_parameters():\n",
    "#                     if 'weight' in name:\n",
    "#                         l2_loss += l2_reg(param) * l2_param\n",
    "                        \n",
    "                _, preds = torch.max(softmax(cls_out).data, 1)\n",
    "                running_clsacc += torch.eq(preds, srclbls.data).float().mean()\n",
    "                running_clsloss += cls_loss.data[0]\n",
    "                steps = steps + 1\n",
    "#                 cls_loss_t = cls_loss + l2_loss\n",
    "                cls_loss_t = cls_loss\n",
    "                cls_opt.zero_grad()\n",
    "                cls_loss_t.backward()\n",
    "                cls_opt.step()\n",
    "            print(\"classification loss: {:.4f}, classification acc: {:.4f}\".\n",
    "                  format(running_clsloss/steps, running_clsacc/steps))\n",
    "                           \n",
    "        for _ in range(num_gen_train):\n",
    "            src_data = iter(dataloaders['train'])\n",
    "            tar_data = get_inifinite_dataloader(dataloaders['val'])\n",
    "            running_critic_loss, running_gen_loss, running_gp_loss, steps = 0., 0., 0., 0\n",
    "            \n",
    "            net = copy.deepcopy(model.features) # copy the network\n",
    "            for params in net.parameters():\n",
    "                params.requires_grad=False\n",
    "            \n",
    "            for srcinps, srclbls in src_data:\n",
    "                tarinps, _ = next(tar_data)\n",
    "\n",
    "                srcinps = srcinps.cuda() if use_gpu else srcinps\n",
    "                srclbls = srclbls.cuda() if use_gpu else srclbls\n",
    "                tarinps = tarinps.cuda() if use_gpu else tarinps\n",
    "                srcinps, srclbls = Variable(srcinps), Variable(srclbls)\n",
    "                tarinps = Variable(tarinps)\n",
    "\n",
    "                real_out = net(srcinps)\n",
    "                gen_out = model.features(tarinps)\n",
    "                critic_flag = random.randint(0,1)\n",
    "                critic_in = torch.cat([real_out, gen_out]) if critic_flag else torch.cat([gen_out, real_out])\n",
    "                critic_out = model._discriminator(critic_in)\n",
    "\n",
    "                D = critic_out[:batch_size] if critic_flag else critic_out[batch_size:]\n",
    "                D_ = critic_out[batch_size:] if critic_flag else critic_out[:batch_size]\n",
    "\n",
    "                critic_loss = (torch.sigmoid(D_) ** 2).mean() + ((1. - torch.sigmoid(D)) ** 2).mean()\n",
    "                gen_loss = D.mean() - D_.mean()\n",
    "                \n",
    "#                 print(\"critic_loss: \", critic_loss.data)\n",
    "#                 print(\"gen_loss: \", gen_loss.data)\n",
    "\n",
    "                running_critic_loss += critic_loss.data[0]\n",
    "                running_gen_loss += gen_loss.data[0]\n",
    "\n",
    "                alpha = torch.Tensor(real_out.size()).uniform_(0,1)\n",
    "                alpha = alpha.cuda() if use_gpu else alpha\n",
    "                diff = real_out.data - gen_out.data\n",
    "                interpolates = real_out.data + (alpha * diff)\n",
    "                interpolates = Variable(interpolates, requires_grad=True)\n",
    "\n",
    "                inter_out = model._discriminator(interpolates)\n",
    "                ones = torch.ones(inter_out.size())\n",
    "                ones = ones.cuda() if use_gpu else ones\n",
    "\n",
    "                grads = autograd.grad(inter_out, interpolates, grad_outputs=ones,\n",
    "                    retain_graph=True, create_graph=True, only_inputs=False)[0]\n",
    "                gp = ((grads.norm(2, dim=1) - 1) ** 2).mean()\n",
    "\n",
    "                running_gp_loss += gp.data[0]\n",
    "#                 print(\"gp_loss: \", gp.data)\n",
    "\n",
    "                for params in model._discriminator.parameters():\n",
    "                    params.requires_grad=True\n",
    "\n",
    "#                 l2_loss = None\n",
    "#                 for name, param in model._discriminator.named_parameters():\n",
    "#                     if 'weight' in name:\n",
    "#                         if l2_loss is None:\n",
    "#                             l2_loss = l2_reg(param) * l2_param\n",
    "#                         else:\n",
    "#                             l2_loss += l2_reg(param) * l2_param\n",
    "\n",
    "#                 critic_loss_t = critic_loss + l2_loss + gp * gp_lambda\n",
    "                critic_loss_t = critic_loss * wd_param + gp * gp_lambda\n",
    "                disc_opt.zero_grad()\n",
    "                critic_loss_t.backward(retain_graph=True)\n",
    "                disc_opt.step()\n",
    "\n",
    "                for params in model._discriminator.parameters():\n",
    "                    params.requires_grad=False\n",
    "\n",
    "#                 l2_loss = None\n",
    "#                 for name, param in model.features.named_parameters():\n",
    "#                     if 'weight' in name:\n",
    "#                         if l2_loss is None:\n",
    "#                             l2_loss = l2_reg(param) * l2_param\n",
    "#                         else:\n",
    "#                             l2_loss += l2_reg(param) * l2_param\n",
    "\n",
    "#                 gen_loss_t = gen_loss + l2_loss\n",
    "                gen_loss_t = gen_loss * g_loss_param\n",
    "\n",
    "                gen_opt.zero_grad()\n",
    "                gen_loss_t.backward()\n",
    "                gen_opt.step()\n",
    "                steps = steps + 1\n",
    "            \n",
    "            del net\n",
    "            \n",
    "        print(\"critic loss: {:.4f}, gen loss: {:.4f}, gp loss: {:.4f}\".\n",
    "              format(running_critic_loss/steps, running_gen_loss/steps, running_gp_loss/steps))\n",
    "            \n",
    "        if lr_schedulers:\n",
    "            for x in lr_schedulers:\n",
    "                if x is not None:\n",
    "                    x.step()\n",
    "\n",
    "#             l2_loss = None\n",
    "#             for name, param in model.classifier.named_parameters():\n",
    "#                 if 'weight' in name:\n",
    "#                     if l2_loss is None:\n",
    "#                         l2_loss = l2_reg(param) * l2_param\n",
    "#                     else:\n",
    "#                         l2_loss += l2_reg(param) * l2_param\n",
    "#             for name, param in model.features.basenet.fc.named_parameters():\n",
    "#                 if 'weight' in name:\n",
    "#                     l2_loss += l2_reg(param) * l2_param\n",
    "\n",
    "#             total_loss = clsloss + l2_loss + g_l * g_loss_param\n",
    "#             total_loss.backward()\n",
    "#             cls_opt.step()\n",
    "\n",
    "        test_model(model, clscriterion, False, None)\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-13T11:28:23.863727Z",
     "start_time": "2018-04-13T11:07:21.554617Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification loss: 3.3006, classification acc: 0.1147\n",
      "classification loss: 3.0096, classification acc: 0.2397\n",
      "critic loss: 0.5056, gen loss: -0.0190, gp loss: 0.8125\n",
      "validation accuracy: 0.3008\n",
      "classification loss: 2.2655, classification acc: 0.5366\n",
      "classification loss: 2.1152, classification acc: 0.5730\n",
      "critic loss: 0.5091, gen loss: -0.0341, gp loss: 0.8120\n",
      "validation accuracy: 0.4049\n",
      "classification loss: 1.7462, classification acc: 0.6484\n",
      "classification loss: 1.6660, classification acc: 0.6664\n",
      "critic loss: 0.5119, gen loss: -0.0456, gp loss: 0.8120\n",
      "validation accuracy: 0.4609\n",
      "classification loss: 1.4605, classification acc: 0.7060\n",
      "classification loss: 1.4109, classification acc: 0.7154\n",
      "critic loss: 0.5148, gen loss: -0.0568, gp loss: 0.8116\n",
      "validation accuracy: 0.4818\n",
      "classification loss: 1.2819, classification acc: 0.7390\n",
      "classification loss: 1.2495, classification acc: 0.7404\n",
      "critic loss: 0.5178, gen loss: -0.0682, gp loss: 0.8111\n",
      "validation accuracy: 0.4922\n",
      "classification loss: 1.1595, classification acc: 0.7525\n",
      "classification loss: 1.1349, classification acc: 0.7562\n",
      "critic loss: 0.5211, gen loss: -0.0792, gp loss: 0.8108\n",
      "validation accuracy: 0.4961\n",
      "classification loss: 1.0701, classification acc: 0.7678\n",
      "classification loss: 1.0530, classification acc: 0.7658\n",
      "critic loss: 0.5247, gen loss: -0.0906, gp loss: 0.8106\n",
      "validation accuracy: 0.5039\n",
      "classification loss: 1.0052, classification acc: 0.7763\n",
      "classification loss: 0.9884, classification acc: 0.7789\n",
      "critic loss: 0.5287, gen loss: -0.1023, gp loss: 0.8099\n",
      "validation accuracy: 0.5156\n",
      "classification loss: 0.9457, classification acc: 0.7841\n",
      "classification loss: 0.9327, classification acc: 0.7864\n",
      "critic loss: 0.5330, gen loss: -0.1135, gp loss: 0.8095\n",
      "validation accuracy: 0.5104\n",
      "classification loss: 0.9013, classification acc: 0.7919\n",
      "classification loss: 0.8906, classification acc: 0.7940\n",
      "critic loss: 0.5381, gen loss: -0.1268, gp loss: 0.8090\n",
      "validation accuracy: 0.5156\n",
      "classification loss: 0.8621, classification acc: 0.8026\n",
      "classification loss: 0.8529, classification acc: 0.8049\n",
      "critic loss: 0.5435, gen loss: -0.1391, gp loss: 0.8085\n",
      "validation accuracy: 0.5221\n",
      "classification loss: 0.8305, classification acc: 0.8033\n",
      "classification loss: 0.8223, classification acc: 0.8052\n",
      "critic loss: 0.5493, gen loss: -0.1515, gp loss: 0.8078\n",
      "validation accuracy: 0.5247\n",
      "classification loss: 0.7954, classification acc: 0.8150\n",
      "classification loss: 0.7897, classification acc: 0.8145\n",
      "critic loss: 0.5557, gen loss: -0.1642, gp loss: 0.8072\n",
      "validation accuracy: 0.5247\n",
      "classification loss: 0.7708, classification acc: 0.8203\n",
      "classification loss: 0.7641, classification acc: 0.8196\n",
      "critic loss: 0.5621, gen loss: -0.1752, gp loss: 0.8065\n",
      "validation accuracy: 0.5352\n",
      "classification loss: 0.7456, classification acc: 0.8239\n",
      "classification loss: 0.7411, classification acc: 0.8251\n",
      "critic loss: 0.5694, gen loss: -0.1881, gp loss: 0.8058\n",
      "validation accuracy: 0.5299\n",
      "classification loss: 0.7228, classification acc: 0.8281\n",
      "classification loss: 0.7173, classification acc: 0.8319\n",
      "critic loss: 0.5769, gen loss: -0.1997, gp loss: 0.8050\n",
      "validation accuracy: 0.5391\n",
      "classification loss: 0.7069, classification acc: 0.8292\n",
      "classification loss: 0.6995, classification acc: 0.8315\n",
      "critic loss: 0.5855, gen loss: -0.2137, gp loss: 0.8045\n",
      "validation accuracy: 0.5378\n",
      "classification loss: 0.6854, classification acc: 0.8342\n",
      "classification loss: 0.6799, classification acc: 0.8343\n",
      "critic loss: 0.5937, gen loss: -0.2247, gp loss: 0.8037\n",
      "validation accuracy: 0.5286\n",
      "classification loss: 0.6633, classification acc: 0.8409\n",
      "classification loss: 0.6586, classification acc: 0.8414\n",
      "critic loss: 0.6034, gen loss: -0.2387, gp loss: 0.8032\n",
      "validation accuracy: 0.5208\n",
      "classification loss: 0.6483, classification acc: 0.8409\n",
      "classification loss: 0.6425, classification acc: 0.8430\n",
      "critic loss: 0.6121, gen loss: -0.2490, gp loss: 0.8026\n",
      "validation accuracy: 0.5195\n",
      "classification loss: 0.6337, classification acc: 0.8434\n",
      "classification loss: 0.6282, classification acc: 0.8443\n",
      "critic loss: 0.6218, gen loss: -0.2604, gp loss: 0.8019\n",
      "validation accuracy: 0.5195\n",
      "classification loss: 0.6181, classification acc: 0.8484\n",
      "classification loss: 0.6187, classification acc: 0.8482\n",
      "critic loss: 0.6243, gen loss: -0.2434, gp loss: 0.8009\n",
      "validation accuracy: 0.5247\n",
      "classification loss: 0.6160, classification acc: 0.8484\n",
      "classification loss: 0.6146, classification acc: 0.8491\n",
      "critic loss: 0.6260, gen loss: -0.2479, gp loss: 0.8009\n",
      "validation accuracy: 0.5130\n",
      "classification loss: 0.6146, classification acc: 0.8484\n",
      "classification loss: 0.6138, classification acc: 0.8491\n",
      "critic loss: 0.6266, gen loss: -0.2467, gp loss: 0.8010\n",
      "validation accuracy: 0.5195\n",
      "classification loss: 0.6111, classification acc: 0.8459\n",
      "classification loss: 0.6122, classification acc: 0.8466\n",
      "critic loss: 0.6269, gen loss: -0.2456, gp loss: 0.8008\n",
      "validation accuracy: 0.5208\n",
      "classification loss: 0.6116, classification acc: 0.8484\n",
      "classification loss: 0.6114, classification acc: 0.8478\n",
      "critic loss: 0.6282, gen loss: -0.2483, gp loss: 0.8009\n",
      "validation accuracy: 0.5208\n",
      "classification loss: 0.6097, classification acc: 0.8512\n",
      "classification loss: 0.6103, classification acc: 0.8498\n",
      "critic loss: 0.6286, gen loss: -0.2467, gp loss: 0.8009\n",
      "validation accuracy: 0.5182\n",
      "classification loss: 0.6104, classification acc: 0.8494\n",
      "classification loss: 0.6071, classification acc: 0.8519\n",
      "critic loss: 0.6299, gen loss: -0.2489, gp loss: 0.8008\n",
      "validation accuracy: 0.5195\n",
      "classification loss: 0.6077, classification acc: 0.8509\n",
      "classification loss: 0.6074, classification acc: 0.8501\n",
      "critic loss: 0.6324, gen loss: -0.2552, gp loss: 0.8008\n",
      "validation accuracy: 0.5156\n",
      "classification loss: 0.6063, classification acc: 0.8459\n",
      "classification loss: 0.6059, classification acc: 0.8484\n",
      "critic loss: 0.6323, gen loss: -0.2521, gp loss: 0.8007\n",
      "validation accuracy: 0.5169\n",
      "classification loss: 0.6037, classification acc: 0.8501\n",
      "classification loss: 0.6044, classification acc: 0.8509\n",
      "critic loss: 0.6334, gen loss: -0.2536, gp loss: 0.8006\n",
      "validation accuracy: 0.5143\n",
      "classification loss: 0.6064, classification acc: 0.8487\n",
      "classification loss: 0.6042, classification acc: 0.8498\n",
      "critic loss: 0.6345, gen loss: -0.2555, gp loss: 0.8004\n",
      "validation accuracy: 0.5195\n",
      "classification loss: 0.6020, classification acc: 0.8505\n",
      "classification loss: 0.6014, classification acc: 0.8517\n",
      "critic loss: 0.6352, gen loss: -0.2548, gp loss: 0.8002\n",
      "validation accuracy: 0.5182\n",
      "classification loss: 0.6004, classification acc: 0.8558\n",
      "classification loss: 0.6005, classification acc: 0.8532\n",
      "critic loss: 0.6370, gen loss: -0.2581, gp loss: 0.8005\n",
      "validation accuracy: 0.5169\n",
      "classification loss: 0.6006, classification acc: 0.8544\n",
      "classification loss: 0.5998, classification acc: 0.8532\n",
      "critic loss: 0.6387, gen loss: -0.2623, gp loss: 0.8003\n",
      "validation accuracy: 0.5234\n",
      "classification loss: 0.5988, classification acc: 0.8477\n",
      "classification loss: 0.5982, classification acc: 0.8493\n",
      "critic loss: 0.6397, gen loss: -0.2631, gp loss: 0.8003\n",
      "validation accuracy: 0.5156\n",
      "classification loss: 0.5977, classification acc: 0.8544\n",
      "classification loss: 0.5973, classification acc: 0.8549\n",
      "critic loss: 0.6401, gen loss: -0.2616, gp loss: 0.8001\n",
      "validation accuracy: 0.5143\n",
      "classification loss: 0.5938, classification acc: 0.8533\n",
      "classification loss: 0.5935, classification acc: 0.8539\n",
      "critic loss: 0.6413, gen loss: -0.2636, gp loss: 0.8001\n",
      "validation accuracy: 0.5143\n",
      "classification loss: 0.5984, classification acc: 0.8519\n",
      "classification loss: 0.5973, classification acc: 0.8516\n",
      "critic loss: 0.6420, gen loss: -0.2637, gp loss: 0.8000\n",
      "validation accuracy: 0.5130\n",
      "classification loss: 0.5932, classification acc: 0.8523\n",
      "classification loss: 0.5927, classification acc: 0.8544\n",
      "critic loss: 0.6439, gen loss: -0.2670, gp loss: 0.7999\n",
      "validation accuracy: 0.5169\n",
      "classification loss: 0.5902, classification acc: 0.8540\n",
      "classification loss: 0.5905, classification acc: 0.8549\n",
      "critic loss: 0.6451, gen loss: -0.2697, gp loss: 0.7998\n",
      "validation accuracy: 0.5091\n",
      "classification loss: 0.5893, classification acc: 0.8516\n",
      "classification loss: 0.5888, classification acc: 0.8532\n",
      "critic loss: 0.6452, gen loss: -0.2669, gp loss: 0.7997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation accuracy: 0.5156\n",
      "classification loss: 0.5926, classification acc: 0.8544\n",
      "classification loss: 0.5917, classification acc: 0.8532\n",
      "critic loss: 0.6454, gen loss: -0.2664, gp loss: 0.8002\n",
      "validation accuracy: 0.5169\n",
      "classification loss: 0.5886, classification acc: 0.8516\n",
      "classification loss: 0.5878, classification acc: 0.8540\n",
      "critic loss: 0.6447, gen loss: -0.2639, gp loss: 0.7999\n",
      "validation accuracy: 0.5169\n",
      "classification loss: 0.5876, classification acc: 0.8537\n",
      "classification loss: 0.5893, classification acc: 0.8535\n",
      "critic loss: 0.6452, gen loss: -0.2654, gp loss: 0.7999\n",
      "validation accuracy: 0.5156\n",
      "classification loss: 0.5896, classification acc: 0.8544\n",
      "classification loss: 0.5894, classification acc: 0.8551\n",
      "critic loss: 0.6451, gen loss: -0.2657, gp loss: 0.7999\n",
      "validation accuracy: 0.5156\n",
      "classification loss: 0.5906, classification acc: 0.8537\n",
      "classification loss: 0.5892, classification acc: 0.8528\n",
      "critic loss: 0.6454, gen loss: -0.2663, gp loss: 0.8001\n",
      "validation accuracy: 0.5143\n",
      "classification loss: 0.5887, classification acc: 0.8516\n",
      "classification loss: 0.5872, classification acc: 0.8535\n",
      "critic loss: 0.6457, gen loss: -0.2675, gp loss: 0.7999\n",
      "validation accuracy: 0.5143\n",
      "classification loss: 0.5923, classification acc: 0.8583\n",
      "classification loss: 0.5893, classification acc: 0.8576\n",
      "critic loss: 0.6462, gen loss: -0.2680, gp loss: 0.7998\n",
      "validation accuracy: 0.5156\n",
      "classification loss: 0.5862, classification acc: 0.8565\n",
      "classification loss: 0.5875, classification acc: 0.8556\n",
      "critic loss: 0.6447, gen loss: -0.2618, gp loss: 0.8001\n",
      "validation accuracy: 0.5195\n",
      "classification loss: 0.5875, classification acc: 0.8540\n",
      "classification loss: 0.5874, classification acc: 0.8544\n",
      "critic loss: 0.6454, gen loss: -0.2644, gp loss: 0.7997\n",
      "validation accuracy: 0.5182\n",
      "classification loss: 0.5924, classification acc: 0.8509\n",
      "classification loss: 0.5903, classification acc: 0.8526\n",
      "critic loss: 0.6448, gen loss: -0.2623, gp loss: 0.8001\n",
      "validation accuracy: 0.5169\n",
      "classification loss: 0.5917, classification acc: 0.8509\n",
      "classification loss: 0.5909, classification acc: 0.8526\n",
      "critic loss: 0.6465, gen loss: -0.2678, gp loss: 0.7998\n",
      "validation accuracy: 0.5182\n",
      "classification loss: 0.5905, classification acc: 0.8544\n",
      "classification loss: 0.5897, classification acc: 0.8544\n",
      "critic loss: 0.6463, gen loss: -0.2667, gp loss: 0.7998\n",
      "validation accuracy: 0.5182\n",
      "classification loss: 0.5866, classification acc: 0.8555\n",
      "classification loss: 0.5865, classification acc: 0.8553\n",
      "critic loss: 0.6463, gen loss: -0.2673, gp loss: 0.7997\n",
      "validation accuracy: 0.5169\n",
      "classification loss: 0.5862, classification acc: 0.8548\n",
      "classification loss: 0.5873, classification acc: 0.8540\n",
      "critic loss: 0.6464, gen loss: -0.2674, gp loss: 0.7999\n",
      "validation accuracy: 0.5091\n",
      "classification loss: 0.5914, classification acc: 0.8516\n",
      "classification loss: 0.5911, classification acc: 0.8535\n",
      "critic loss: 0.6456, gen loss: -0.2638, gp loss: 0.7997\n",
      "validation accuracy: 0.5091\n",
      "classification loss: 0.5922, classification acc: 0.8491\n",
      "classification loss: 0.5896, classification acc: 0.8519\n",
      "critic loss: 0.6463, gen loss: -0.2666, gp loss: 0.7999\n",
      "validation accuracy: 0.5156\n",
      "classification loss: 0.5883, classification acc: 0.8551\n",
      "classification loss: 0.5892, classification acc: 0.8532\n",
      "critic loss: 0.6468, gen loss: -0.2676, gp loss: 0.8000\n",
      "validation accuracy: 0.5117\n",
      "classification loss: 0.5881, classification acc: 0.8533\n",
      "classification loss: 0.5891, classification acc: 0.8519\n",
      "critic loss: 0.6467, gen loss: -0.2668, gp loss: 0.7997\n",
      "validation accuracy: 0.5104\n",
      "classification loss: 0.5873, classification acc: 0.8512\n",
      "classification loss: 0.5885, classification acc: 0.8516\n",
      "critic loss: 0.6471, gen loss: -0.2681, gp loss: 0.7998\n",
      "validation accuracy: 0.5130\n",
      "classification loss: 0.5875, classification acc: 0.8540\n",
      "classification loss: 0.5879, classification acc: 0.8539\n",
      "critic loss: 0.6475, gen loss: -0.2698, gp loss: 0.7998\n",
      "validation accuracy: 0.5104\n",
      "classification loss: 0.5885, classification acc: 0.8512\n",
      "classification loss: 0.5872, classification acc: 0.8523\n",
      "critic loss: 0.6468, gen loss: -0.2672, gp loss: 0.7999\n",
      "validation accuracy: 0.5143\n",
      "classification loss: 0.5874, classification acc: 0.8548\n",
      "classification loss: 0.5874, classification acc: 0.8533\n",
      "critic loss: 0.6477, gen loss: -0.2699, gp loss: 0.7998\n",
      "validation accuracy: 0.5156\n",
      "classification loss: 0.5875, classification acc: 0.8565\n",
      "classification loss: 0.5877, classification acc: 0.8565\n",
      "critic loss: 0.6463, gen loss: -0.2649, gp loss: 0.7998\n",
      "validation accuracy: 0.5143\n",
      "classification loss: 0.5872, classification acc: 0.8512\n",
      "classification loss: 0.5875, classification acc: 0.8526\n",
      "critic loss: 0.6469, gen loss: -0.2677, gp loss: 0.7997\n",
      "validation accuracy: 0.5130\n",
      "classification loss: 0.5874, classification acc: 0.8512\n",
      "classification loss: 0.5876, classification acc: 0.8528\n",
      "critic loss: 0.6466, gen loss: -0.2664, gp loss: 0.7999\n",
      "validation accuracy: 0.5156\n",
      "classification loss: 0.5827, classification acc: 0.8540\n",
      "classification loss: 0.5854, classification acc: 0.8533\n",
      "critic loss: 0.6468, gen loss: -0.2662, gp loss: 0.7998\n",
      "validation accuracy: 0.5130\n",
      "classification loss: 0.5881, classification acc: 0.8523\n",
      "classification loss: 0.5885, classification acc: 0.8530\n",
      "critic loss: 0.6467, gen loss: -0.2668, gp loss: 0.7997\n",
      "validation accuracy: 0.5195\n",
      "classification loss: 0.5850, classification acc: 0.8509\n",
      "classification loss: 0.5865, classification acc: 0.8514\n",
      "critic loss: 0.6461, gen loss: -0.2640, gp loss: 0.7999\n",
      "validation accuracy: 0.5182\n",
      "classification loss: 0.5861, classification acc: 0.8558\n",
      "classification loss: 0.5876, classification acc: 0.8548\n",
      "critic loss: 0.6476, gen loss: -0.2696, gp loss: 0.8000\n",
      "validation accuracy: 0.5221\n",
      "classification loss: 0.5871, classification acc: 0.8551\n",
      "classification loss: 0.5867, classification acc: 0.8542\n",
      "critic loss: 0.6471, gen loss: -0.2674, gp loss: 0.7997\n",
      "validation accuracy: 0.5182\n",
      "classification loss: 0.5875, classification acc: 0.8519\n",
      "classification loss: 0.5875, classification acc: 0.8523\n",
      "critic loss: 0.6468, gen loss: -0.2660, gp loss: 0.7996\n",
      "validation accuracy: 0.5169\n",
      "classification loss: 0.5867, classification acc: 0.8551\n",
      "classification loss: 0.5872, classification acc: 0.8556\n",
      "critic loss: 0.6469, gen loss: -0.2673, gp loss: 0.7998\n",
      "validation accuracy: 0.5117\n",
      "classification loss: 0.5912, classification acc: 0.8523\n",
      "classification loss: 0.5897, classification acc: 0.8537\n",
      "critic loss: 0.6477, gen loss: -0.2695, gp loss: 0.7997\n",
      "validation accuracy: 0.5195\n",
      "Training complete in 20m 55s\n",
      "validation accuracy: 0.5143\n",
      "validation accuracy: 0.5156\n",
      "validation accuracy: 0.5130\n",
      "validation accuracy: 0.5182\n",
      "validation accuracy: 0.5208\n"
     ]
    }
   ],
   "source": [
    "train_model(model_ft, clscriterion, disc_opt, gen_opt, cls_opt, lr_schedulers, num_epochs=EPOCHS)\n",
    "\n",
    "save_name = \"grl_model_with_transform.pth\"\n",
    "test_model(model_ft, clscriterion, False, save_name)\n",
    "test_model(model_ft, clscriterion, False, save_name)\n",
    "test_model(model_ft, clscriterion, False, save_name)\n",
    "test_model(model_ft, clscriterion, False, save_name)\n",
    "test_model(model_ft, clscriterion, False, save_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
