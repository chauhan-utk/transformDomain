{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T04:51:20.261006Z",
     "start_time": "2018-04-19T04:51:20.257420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%set_env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%set_env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T04:51:23.445849Z",
     "start_time": "2018-04-19T04:51:21.293185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('disk I/O error',)).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from config import domainData\n",
    "from config import num_classes as NUM_CLASSES\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "import utils\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from localresponsenorm import LocalResponseNorm\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "net = models.alexnet(pretrained=True)\n",
    "net.classifier = nn.Sequential(*list(net.classifier.children())[:-1])\n",
    "# print(net)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T04:52:02.571469Z",
     "start_time": "2018-04-19T04:52:01.897569Z"
    }
   },
   "outputs": [],
   "source": [
    "class LambdaBase(nn.Sequential):\n",
    "    def __init__(self, fn, *args):\n",
    "        super(LambdaBase, self).__init__(*args)\n",
    "        self.lambda_func = fn\n",
    "\n",
    "    def forward_prepare(self, input):\n",
    "        output = []\n",
    "        for module in self._modules.values():\n",
    "            output.append(module(input))\n",
    "        return output if output else input\n",
    "\n",
    "class Lambda(LambdaBase):\n",
    "    def forward(self, input):\n",
    "        return self.lambda_func(self.forward_prepare(input))\n",
    "    \n",
    "alexnet = nn.Sequential( # Sequential,\n",
    "    nn.Conv2d(3,96,(11, 11),(4, 4)),\n",
    "    nn.\n",
    "    nn.ReLU(inplace=True),\n",
    "#     LocalResponseNorm(5, 0.0001, 0.75, 1),\n",
    "    nn.MaxPool2d((3, 3),(2, 2),(0, 0),ceil_mode=True),\n",
    "    nn.Conv2d(96,256,(5, 5),(1, 1),(2, 2),1,2),\n",
    "    nn.ReLU(inplace=True),\n",
    "#     LocalResponseNorm(5, 0.0001, 0.75, 1),\n",
    "    nn.MaxPool2d((3, 3),(2, 2),(0, 0),ceil_mode=True),\n",
    "    nn.Conv2d(256,384,(3, 3),(1, 1),(1, 1)),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(384,384,(3, 3),(1, 1),(1, 1),1,2),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Conv2d(384,256,(3, 3),(1, 1),(1, 1),1,2),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.MaxPool2d((3, 3),(2, 2),(0, 0),ceil_mode=True),\n",
    "    Lambda(lambda x: x.view(x.size(0),-1)), # View,\n",
    "    nn.Sequential(Lambda(lambda x: x.view(1,-1) if 1==len(x.size()) else x ),nn.Linear(9216,4096)), # Linear,\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Sequential(Lambda(lambda x: x.view(1,-1) if 1==len(x.size()) else x ),nn.Linear(4096,4096)), # Linear,\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Sequential(Lambda(lambda x: x.view(1,-1) if 1==len(x.size()) else x ),nn.Linear(4096,1000)), # Linear,\n",
    "    nn.Softmax(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T04:52:06.949951Z",
     "start_time": "2018-04-19T04:52:06.394066Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = alexnet\n",
    "net.load_state_dict(torch.load('alexnet.pth'))\n",
    "# print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T04:52:10.096463Z",
     "start_time": "2018-04-19T04:52:10.092645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(3, 96, kernel_size=(11, 11), stride=(4, 4))\n",
      "  (1): ReLU(inplace)\n",
      "  (2): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=1)\n",
      "  (3): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=True)\n",
      "  (4): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=2)\n",
      "  (5): ReLU(inplace)\n",
      "  (6): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=1)\n",
      "  (7): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=True)\n",
      "  (8): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (9): ReLU(inplace)\n",
      "  (10): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2)\n",
      "  (11): ReLU(inplace)\n",
      "  (12): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2)\n",
      "  (13): ReLU(inplace)\n",
      "  (14): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), dilation=(1, 1), ceil_mode=True)\n",
      "  (15): Lambda(\n",
      "  )\n",
      "  (16): Sequential(\n",
      "    (0): Lambda(\n",
      "    )\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "  )\n",
      "  (17): ReLU(inplace)\n",
      "  (18): Dropout(p=0.5)\n",
      "  (19): Sequential(\n",
      "    (0): Lambda(\n",
      "    )\n",
      "    (1): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  )\n",
      "  (20): ReLU(inplace)\n",
      "  (21): Dropout(p=0.5)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential(*list(net.children())[:-2])\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T04:52:48.416465Z",
     "start_time": "2018-04-19T04:52:48.411834Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = torch.Tensor(1,3,227,227).uniform_(0,255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-19T04:52:50.269630Z",
     "start_time": "2018-04-19T04:52:49.939146Z"
    }
   },
   "outputs": [],
   "source": [
    "b = net(Variable(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T13:58:43.272646Z",
     "start_time": "2018-04-17T13:58:43.108093Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src = domainData['amazon']\n",
    "tar = domainData['webcam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T13:58:43.463379Z",
     "start_time": "2018-04-17T13:58:43.274259Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomResizedCrop(227, scale=(0.50,1.0), ratio=(1.,1.)),\n",
    "    transforms.RandomHorizontalFlip()\n",
    "])\n",
    "tmp_dataset = datasets.ImageFolder(src, transform=tmp_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T13:58:43.646852Z",
     "start_time": "2018-04-17T13:58:43.464575Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "tmp_len = len(tmp_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T13:58:43.825755Z",
     "start_time": "2018-04-17T13:58:43.648792Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOMAAADjCAIAAAD47DQbAAA7d0lEQVR4nO29Z3Mk6XUl/KT3PrN8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\nrkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=227x227 at 0x7F0FA6D81B70>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_dataset[random.randint(0,tmp_len-1)][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T13:58:43.961066Z",
     "start_time": "2018-04-17T13:58:43.826983Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "#     transforms.CenterCrop(227),\n",
    "#     transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomResizedCrop(227, scale=(0.50,1.0), ratio=(1.,1.)),\n",
    "#     transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[1.,1.,1.]),\n",
    "    transforms.Lambda(lambda x: torch.stack([x[2],x[1],x[0]])), # RGB -> BGR\n",
    "    transforms.Lambda(lambda x: x * 255.)\n",
    "])\n",
    "tar_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(227),\n",
    "#     transforms.RandomResizedCrop(224, scale=(0.25,1.0)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                        std=[1.,1.,1.]),\n",
    "    transforms.Lambda(lambda x: torch.stack([x[2],x[1],x[0]])),\n",
    "    transforms.Lambda(lambda x: x * 255.)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T13:58:44.183947Z",
     "start_time": "2018-04-17T13:58:43.962379Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_dataset = datasets.ImageFolder(src, transform=src_transforms)\n",
    "tar_dataset = datasets.ImageFolder(tar, transform=tar_transforms)\n",
    "srcSampler = torch.utils.data.sampler.RandomSampler(src_dataset)\n",
    "tarSampler = torch.utils.data.sampler.RandomSampler(tar_dataset)\n",
    "srcDataLen = len(src_dataset)\n",
    "tarDataLen = len(tar_dataset)\n",
    "use_gpu = True and torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T13:58:44.285191Z",
     "start_time": "2018-04-17T13:58:44.185660Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "opt = {\n",
    "    'src': 'Amazon',\n",
    "    'tar': 'Webcam',\n",
    "    'manual_seed':1,\n",
    "    'batchSize':64,\n",
    "    'use_gpu': use_gpu,\n",
    "    'num_classes': 31,\n",
    "    'epochs': 250,\n",
    "    'momentum': 0.9,\n",
    "    'lr': 2e-4,\n",
    "    'lr_sch': 0,\n",
    "    'lr_sch_gamma': 0.1,\n",
    "    'p_lr_decay': 250,\n",
    "    'n0': 1.,\n",
    "    'alpha': 10,\n",
    "    'beta': 0.75,\n",
    "    'betas': (0.5,0.99),\n",
    "    'net_wtDcy': 0.001,\n",
    "    'btl_wtDcy': 0.001,\n",
    "    'srcDataLen': srcDataLen,\n",
    "    'tarDataLen': tarDataLen\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "experiment.log_multiple_params(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T13:58:44.545030Z",
     "start_time": "2018-04-17T13:58:44.287281Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(opt['manual_seed'])\n",
    "if opt['use_gpu']: torch.cuda.manual_seed(opt['manual_seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T13:58:44.702082Z",
     "start_time": "2018-04-17T13:58:44.546650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_gpu:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"use_gpu: \", opt['use_gpu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T13:58:44.856409Z",
     "start_time": "2018-04-17T13:58:44.704060Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src_dataloader = torch.utils.data.DataLoader(src_dataset, batch_size=opt['batchSize'], \n",
    "                                             shuffle=(srcSampler is None), sampler=srcSampler,\n",
    "                                            num_workers=2, pin_memory=True, drop_last=False)\n",
    "tar_dataloader = torch.utils.data.DataLoader(tar_dataset, batch_size=opt['batchSize'],\n",
    "                                            shuffle=(tarSampler is None), sampler=tarSampler,\n",
    "                                            num_workers=2, pin_memory=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T13:58:45.044083Z",
     "start_time": "2018-04-17T13:58:44.858129Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "#     print(m)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.xavier_normal(m.weight)\n",
    "        init.constant(m.bias, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T13:58:45.200697Z",
     "start_time": "2018-04-17T13:58:45.045820Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.classifier = nn.Sequential(nn.Linear(4096,256),\n",
    "                                       nn.ReLU(inplace=True), nn.Linear(256, opt['num_classes']))\n",
    "        self.classifier.apply(init_weights)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T13:58:45.530694Z",
     "start_time": "2018-04-17T13:58:45.201992Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net2 = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T13:58:48.230254Z",
     "start_time": "2018-04-17T13:58:45.531994Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if opt['use_gpu']:\n",
    "    net = net.cuda()\n",
    "    net2 = net2.cuda()\n",
    "    torch.backends.cudnn.enabled=True\n",
    "    torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To computer norm of various parameters\n",
    "\n",
    "```python\n",
    "for name, param in net2.named_parameters():\n",
    "    nrm = torch.norm(param, 2)\n",
    "    zero = param.eq(0.).float().sum()\n",
    "    nele = torch.numel(param)\n",
    "    print(name, nrm.data[0], nele, zero.data[0])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T13:58:48.234231Z",
     "start_time": "2018-04-17T13:58:48.231656Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "softmax = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T13:58:48.433069Z",
     "start_time": "2018-04-17T13:58:48.235467Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "netfeatwt, netfeatbias = list(), list()\n",
    "for name, param in net.named_parameters():\n",
    "    if name in ['10.weight', '12.weight', '16.1.weight', '19.1.weight']: netfeatwt.append(param)\n",
    "    elif name in ['10.bias', '12.bias', '16.1.bias', '19.1.bias']: netfeatbias.append(param)\n",
    "    else: param.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T13:58:48.603267Z",
     "start_time": "2018-04-17T13:58:48.434965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight False\n",
      "0.bias False\n",
      "4.weight False\n",
      "4.bias False\n",
      "8.weight False\n",
      "8.bias False\n",
      "10.weight True\n",
      "10.bias True\n",
      "12.weight True\n",
      "12.bias True\n",
      "16.1.weight True\n",
      "16.1.bias True\n",
      "19.1.weight True\n",
      "19.1.bias True\n"
     ]
    }
   ],
   "source": [
    "for name, param in net.named_parameters():\n",
    "    print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T13:58:48.755743Z",
     "start_time": "2018-04-17T13:58:48.604734Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net2_weight, net2_bias = list(), list()\n",
    "for name, param in net2.named_parameters():\n",
    "    if 'weight' in name: net2_weight.append(param)\n",
    "    elif 'bias' in name: net2_bias.append(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T13:58:48.924048Z",
     "start_time": "2018-04-17T13:58:48.757598Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd_params = [\n",
    "    {'params': netfeatwt, 'lr': opt['lr'], 'momentum': opt['momentum'], 'weight_decay': opt['net_wtDcy'], 'lr_mul': 1., 'name': 'netfeatwt'},\n",
    "    {'params': netfeatbias, 'lr': opt['lr'], 'momentum': opt['momentum'], 'weight_decay': 0*opt['net_wtDcy'], 'lr_mul': 2., 'name': 'netfeatbias'},\n",
    "    {'params': net2_weight, 'lr': opt['lr'], 'momentum': opt['momentum'], 'weight_decay': opt['btl_wtDcy'], 'lr_mul': 10., 'name': 'net2wt'},\n",
    "    {'params': net2_bias, 'lr': opt['lr'], 'momentum': opt['momentum'], 'weight_decay': 0*opt['btl_wtDcy'], 'lr_mul': 20., 'name': 'net2bias'}\n",
    "]\n",
    "\n",
    "# optimizer = optim.Adam(net2.parameters(), lr=opt['lr'], betas=opt['betas'], weight_decay=opt['weight_decay'])\n",
    "# optimizer = optim.Adam(sgd_params, betas=opt['betas'])\n",
    "\n",
    "optimizer_2 = optim.SGD(sgd_params)\n",
    "# optimizer_2 = optim.SGD(net2.parameters(), lr=opt['lr'], momentum=opt['momentum'], weight_decay=opt['btl_wtDcy'])\n",
    "\n",
    "lr_sch = None\n",
    "if opt['lr_sch']=='step': lr_sch = optim.lr_scheduler.StepLR(optimizer, 20, opt['lr_sch_gamma'])\n",
    "if opt['lr_sch']=='exponential': lr_sch = optim.lr_scheduler.ExponentialLR(optimizer_2, opt['lr_sch_gamma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T13:58:49.108187Z",
     "start_time": "2018-04-17T13:58:48.925686Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_validation_acc(cm=False):\n",
    "    tarData = iter(tar_dataloader)\n",
    "    totalCorrects = 0.\n",
    "    if cm: y_preds, y_true = list(), list()\n",
    "    for tarimgs, tarlbls in tarData:\n",
    "        tarimgs = tarimgs.cuda() if opt['use_gpu'] else tarimgs\n",
    "        tarlbls = tarlbls.cuda() if opt['use_gpu'] else tarlbls      \n",
    "        tarimgs = Variable(tarimgs, volatile=True)\n",
    "\n",
    "        feat_ = net(tarimgs)\n",
    "        logits = net2(feat_)\n",
    "\n",
    "        _, preds = torch.max(softmax(logits).data, 1)\n",
    "        totalCorrects += torch.eq(preds, tarlbls).float().sum()\n",
    "        if cm: y_preds.append(preds.cpu().numpy()), y_true.append(tarlbls.cpu().numpy())\n",
    "    valAcc = totalCorrects / opt['tarDataLen']\n",
    "    if cm: return valAcc, y_preds, y_true\n",
    "    return valAcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-17T14:37:06.622312Z",
     "start_time": "2018-04-17T13:58:49.109821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_p:  0.971012890912\n",
      "Epoch: 1, train loss: 2.8401, train acc: 0.3536, validation acc: 0.4239\n",
      "n_p:  0.94391346936\n",
      "Epoch: 2, train loss: 1.3085, train acc: 0.6383, validation acc: 0.4541\n",
      "n_p:  0.918515486359\n",
      "Epoch: 3, train loss: 1.1293, train acc: 0.6894, validation acc: 0.4843\n",
      "n_p:  0.894656884184\n",
      "Epoch: 4, train loss: 0.8999, train acc: 0.7501, validation acc: 0.4818\n",
      "n_p:  0.872195949493\n",
      "Epoch: 5, train loss: 0.8293, train acc: 0.7668, validation acc: 0.5031\n",
      "n_p:  0.851008182997\n",
      "Epoch: 6, train loss: 0.7816, train acc: 0.7849, validation acc: 0.5057\n",
      "n_p:  0.830983733898\n",
      "Epoch: 7, train loss: 0.6973, train acc: 0.7987, validation acc: 0.5069\n",
      "n_p:  0.812025283127\n",
      "Epoch: 8, train loss: 0.5958, train acc: 0.8271, validation acc: 0.5157\n",
      "n_p:  0.794046285931\n",
      "Epoch: 9, train loss: 0.5129, train acc: 0.8477, validation acc: 0.5019\n",
      "n_p:  0.776969504241\n",
      "Epoch: 10, train loss: 0.5115, train acc: 0.8463, validation acc: 0.4994\n",
      "n_p:  0.760725774313\n",
      "Epoch: 11, train loss: 0.4771, train acc: 0.8537, validation acc: 0.5145\n",
      "n_p:  0.745252966542\n",
      "Epoch: 12, train loss: 0.4007, train acc: 0.8733, validation acc: 0.5270\n",
      "n_p:  0.730495103218\n",
      "Epoch: 13, train loss: 0.3951, train acc: 0.8775, validation acc: 0.5333\n",
      "n_p:  0.716401606779\n",
      "Epoch: 14, train loss: 0.3933, train acc: 0.8846, validation acc: 0.5182\n",
      "n_p:  0.702926656488\n",
      "Epoch: 15, train loss: 0.4219, train acc: 0.8750, validation acc: 0.5321\n",
      "n_p:  0.690028635604\n",
      "Epoch: 16, train loss: 0.4213, train acc: 0.8779, validation acc: 0.4918\n",
      "n_p:  0.677669654479\n",
      "Epoch: 17, train loss: 0.3481, train acc: 0.8903, validation acc: 0.5094\n",
      "n_p:  0.665815137616\n",
      "Epoch: 18, train loss: 0.2932, train acc: 0.9105, validation acc: 0.5358\n",
      "n_p:  0.654433464847\n",
      "Epoch: 19, train loss: 0.2960, train acc: 0.9063, validation acc: 0.5346\n",
      "n_p:  0.643495658493\n",
      "Epoch: 20, train loss: 0.2722, train acc: 0.9152, validation acc: 0.5283\n",
      "n_p:  0.632975109741\n",
      "Epoch: 21, train loss: 0.2558, train acc: 0.9240, validation acc: 0.5170\n",
      "n_p:  0.622847338579\n",
      "Epoch: 22, train loss: 0.2636, train acc: 0.9286, validation acc: 0.5182\n",
      "n_p:  0.61308978258\n",
      "Epoch: 23, train loss: 0.4497, train acc: 0.8832, validation acc: 0.5409\n",
      "n_p:  0.60368161052\n",
      "Epoch: 24, train loss: 0.3009, train acc: 0.9159, validation acc: 0.5195\n",
      "n_p:  0.594603557501\n",
      "Epoch: 25, train loss: 0.2621, train acc: 0.9233, validation acc: 0.5421\n",
      "n_p:  0.585837778696\n",
      "Epoch: 26, train loss: 0.2555, train acc: 0.9201, validation acc: 0.5333\n",
      "n_p:  0.577367719316\n",
      "Epoch: 27, train loss: 0.2617, train acc: 0.9184, validation acc: 0.5447\n",
      "n_p:  0.569177998713\n",
      "Epoch: 28, train loss: 0.1930, train acc: 0.9397, validation acc: 0.5560\n",
      "n_p:  0.561254306855\n",
      "Epoch: 29, train loss: 0.2095, train acc: 0.9336, validation acc: 0.5409\n",
      "n_p:  0.55358331165\n",
      "Epoch: 30, train loss: 0.1845, train acc: 0.9450, validation acc: 0.5409\n",
      "n_p:  0.546152575809\n",
      "Epoch: 31, train loss: 0.1828, train acc: 0.9421, validation acc: 0.5572\n",
      "n_p:  0.538950482107\n",
      "Epoch: 32, train loss: 0.1892, train acc: 0.9436, validation acc: 0.5610\n",
      "n_p:  0.531966166079\n",
      "Epoch: 33, train loss: 0.1542, train acc: 0.9514, validation acc: 0.5459\n",
      "n_p:  0.525189455285\n",
      "Epoch: 34, train loss: 0.1588, train acc: 0.9514, validation acc: 0.5434\n",
      "n_p:  0.518610814407\n",
      "Epoch: 35, train loss: 0.1647, train acc: 0.9471, validation acc: 0.5396\n",
      "n_p:  0.512221295536\n",
      "Epoch: 36, train loss: 0.1561, train acc: 0.9492, validation acc: 0.5472\n",
      "n_p:  0.506012493073\n",
      "Epoch: 37, train loss: 0.1483, train acc: 0.9549, validation acc: 0.5384\n",
      "n_p:  0.499976502761\n",
      "Epoch: 38, train loss: 0.2080, train acc: 0.9524, validation acc: 0.5472\n",
      "n_p:  0.494105884401\n",
      "Epoch: 39, train loss: 0.2913, train acc: 0.9180, validation acc: 0.5132\n",
      "n_p:  0.488393627875\n",
      "Epoch: 40, train loss: 0.1654, train acc: 0.9528, validation acc: 0.5321\n",
      "n_p:  0.482833122129\n",
      "Epoch: 41, train loss: 0.1382, train acc: 0.9585, validation acc: 0.5245\n",
      "n_p:  0.477418126833\n",
      "Epoch: 42, train loss: 0.1646, train acc: 0.9599, validation acc: 0.5447\n",
      "n_p:  0.472142746435\n",
      "Epoch: 43, train loss: 0.1866, train acc: 0.9400, validation acc: 0.5283\n",
      "n_p:  0.46700140638\n",
      "Epoch: 44, train loss: 0.1368, train acc: 0.9595, validation acc: 0.5484\n",
      "n_p:  0.461988831292\n",
      "Epoch: 45, train loss: 0.1515, train acc: 0.9553, validation acc: 0.5434\n",
      "n_p:  0.457100024919\n",
      "Epoch: 46, train loss: 0.1321, train acc: 0.9595, validation acc: 0.5560\n",
      "n_p:  0.452330251689\n",
      "Epoch: 47, train loss: 0.1151, train acc: 0.9649, validation acc: 0.5597\n",
      "n_p:  0.44767501972\n",
      "Epoch: 48, train loss: 0.1233, train acc: 0.9627, validation acc: 0.5358\n",
      "n_p:  0.443130065144\n",
      "Epoch: 49, train loss: 0.1188, train acc: 0.9677, validation acc: 0.5597\n",
      "n_p:  0.438691337651\n",
      "Epoch: 50, train loss: 0.1071, train acc: 0.9656, validation acc: 0.5509\n",
      "n_p:  0.434354987111\n",
      "Epoch: 51, train loss: 0.1043, train acc: 0.9702, validation acc: 0.5535\n",
      "n_p:  0.430117351209\n",
      "Epoch: 52, train loss: 0.1099, train acc: 0.9620, validation acc: 0.5447\n",
      "n_p:  0.425974943991\n",
      "Epoch: 53, train loss: 0.2204, train acc: 0.9695, validation acc: 0.5484\n",
      "n_p:  0.421924445237\n",
      "Epoch: 54, train loss: 0.2677, train acc: 0.9272, validation acc: 0.5421\n",
      "n_p:  0.41796269061\n",
      "Epoch: 55, train loss: 0.1358, train acc: 0.9595, validation acc: 0.5447\n",
      "n_p:  0.4140866625\n",
      "Epoch: 56, train loss: 0.1207, train acc: 0.9666, validation acc: 0.5296\n",
      "n_p:  0.410293481508\n",
      "Epoch: 57, train loss: 0.1355, train acc: 0.9620, validation acc: 0.5308\n",
      "n_p:  0.406580398527\n",
      "Epoch: 58, train loss: 0.1167, train acc: 0.9634, validation acc: 0.5447\n",
      "n_p:  0.402944787364\n",
      "Epoch: 59, train loss: 0.1051, train acc: 0.9677, validation acc: 0.5447\n",
      "n_p:  0.399384137858\n",
      "Epoch: 60, train loss: 0.1024, train acc: 0.9691, validation acc: 0.5610\n",
      "n_p:  0.395896049465\n",
      "Epoch: 61, train loss: 0.0867, train acc: 0.9748, validation acc: 0.5434\n",
      "n_p:  0.392478225261\n",
      "Epoch: 62, train loss: 0.1155, train acc: 0.9670, validation acc: 0.5572\n",
      "n_p:  0.389128466346\n",
      "Epoch: 63, train loss: 0.0916, train acc: 0.9727, validation acc: 0.5283\n",
      "n_p:  0.385844666598\n",
      "Epoch: 64, train loss: 0.0892, train acc: 0.9727, validation acc: 0.5547\n",
      "n_p:  0.382624807777\n",
      "Epoch: 65, train loss: 0.0977, train acc: 0.9716, validation acc: 0.5535\n",
      "n_p:  0.379466954924\n",
      "Epoch: 66, train loss: 0.0953, train acc: 0.9691, validation acc: 0.5283\n",
      "n_p:  0.376369252062\n",
      "Epoch: 67, train loss: 0.0825, train acc: 0.9755, validation acc: 0.5623\n",
      "n_p:  0.373329918151\n",
      "Epoch: 68, train loss: 0.0772, train acc: 0.9783, validation acc: 0.5686\n",
      "n_p:  0.370347243299\n",
      "Epoch: 69, train loss: 0.1016, train acc: 0.9663, validation acc: 0.5547\n",
      "n_p:  0.367419585202\n",
      "Epoch: 70, train loss: 0.0830, train acc: 0.9744, validation acc: 0.5396\n",
      "n_p:  0.36454536579\n",
      "Epoch: 71, train loss: 0.0849, train acc: 0.9748, validation acc: 0.5371\n",
      "n_p:  0.36172306808\n",
      "Epoch: 72, train loss: 0.0781, train acc: 0.9762, validation acc: 0.5572\n",
      "n_p:  0.358951233214\n",
      "Epoch: 73, train loss: 0.0825, train acc: 0.9744, validation acc: 0.5308\n",
      "n_p:  0.356228457661\n",
      "Epoch: 74, train loss: 0.0858, train acc: 0.9759, validation acc: 0.5509\n",
      "n_p:  0.353553390593\n",
      "Epoch: 75, train loss: 0.0747, train acc: 0.9798, validation acc: 0.5472\n",
      "n_p:  0.350924731396\n",
      "Epoch: 76, train loss: 0.0812, train acc: 0.9773, validation acc: 0.5572\n",
      "n_p:  0.348341227332\n",
      "Epoch: 77, train loss: 0.0747, train acc: 0.9759, validation acc: 0.5434\n",
      "n_p:  0.345801671329\n",
      "Epoch: 78, train loss: 0.0896, train acc: 0.9737, validation acc: 0.5421\n",
      "n_p:  0.343304899892\n",
      "Epoch: 79, train loss: 0.0845, train acc: 0.9698, validation acc: 0.5421\n",
      "n_p:  0.340849791129\n",
      "Epoch: 80, train loss: 0.0784, train acc: 0.9776, validation acc: 0.5597\n",
      "n_p:  0.338435262886\n",
      "Epoch: 81, train loss: 0.1037, train acc: 0.9762, validation acc: 0.5358\n",
      "n_p:  0.336060270978\n",
      "Epoch: 82, train loss: 0.1394, train acc: 0.9663, validation acc: 0.5296\n",
      "n_p:  0.333723807519\n",
      "Epoch: 83, train loss: 0.1017, train acc: 0.9673, validation acc: 0.5421\n",
      "n_p:  0.331424899338\n",
      "Epoch: 84, train loss: 0.0781, train acc: 0.9744, validation acc: 0.5610\n",
      "n_p:  0.329162606481\n",
      "Epoch: 85, train loss: 0.0937, train acc: 0.9698, validation acc: 0.5560\n",
      "n_p:  0.326936020781\n",
      "Epoch: 86, train loss: 0.0736, train acc: 0.9787, validation acc: 0.5635\n",
      "n_p:  0.324744264515\n",
      "Epoch: 87, train loss: 0.0735, train acc: 0.9794, validation acc: 0.5648\n",
      "n_p:  0.32258648912\n",
      "Epoch: 88, train loss: 0.0630, train acc: 0.9823, validation acc: 0.5472\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_p:  0.320461873978\n",
      "Epoch: 89, train loss: 0.0591, train acc: 0.9819, validation acc: 0.5761\n",
      "n_p:  0.318369625258\n",
      "Epoch: 90, train loss: 0.0765, train acc: 0.9783, validation acc: 0.5384\n",
      "n_p:  0.316308974821\n",
      "Epoch: 91, train loss: 0.0873, train acc: 0.9730, validation acc: 0.5522\n",
      "n_p:  0.314279179173\n",
      "Epoch: 92, train loss: 0.0719, train acc: 0.9776, validation acc: 0.5623\n",
      "n_p:  0.312279518475\n",
      "Epoch: 93, train loss: 0.0684, train acc: 0.9766, validation acc: 0.5686\n",
      "n_p:  0.310309295593\n",
      "Epoch: 94, train loss: 0.0659, train acc: 0.9791, validation acc: 0.5509\n",
      "n_p:  0.308367835205\n",
      "Epoch: 95, train loss: 0.0556, train acc: 0.9844, validation acc: 0.5497\n",
      "n_p:  0.306454482938\n",
      "Epoch: 96, train loss: 0.0657, train acc: 0.9812, validation acc: 0.5547\n",
      "n_p:  0.304568604553\n",
      "Epoch: 97, train loss: 0.0607, train acc: 0.9805, validation acc: 0.5786\n",
      "n_p:  0.30270958517\n",
      "Epoch: 98, train loss: 0.0636, train acc: 0.9787, validation acc: 0.5484\n",
      "n_p:  0.300876828521\n",
      "Epoch: 99, train loss: 0.0639, train acc: 0.9791, validation acc: 0.5497\n",
      "n_p:  0.299069756244\n",
      "Epoch: 100, train loss: 0.0638, train acc: 0.9819, validation acc: 0.5572\n",
      "n_p:  0.297287807209\n",
      "Epoch: 101, train loss: 0.0611, train acc: 0.9798, validation acc: 0.5522\n",
      "n_p:  0.295530436869\n",
      "Epoch: 102, train loss: 0.0577, train acc: 0.9851, validation acc: 0.5686\n",
      "n_p:  0.293797116647\n",
      "Epoch: 103, train loss: 0.0650, train acc: 0.9791, validation acc: 0.5648\n",
      "n_p:  0.292087333349\n",
      "Epoch: 104, train loss: 0.0575, train acc: 0.9833, validation acc: 0.5711\n",
      "n_p:  0.290400588595\n",
      "Epoch: 105, train loss: 0.0569, train acc: 0.9808, validation acc: 0.5560\n",
      "n_p:  0.288736398289\n",
      "Epoch: 106, train loss: 0.0474, train acc: 0.9854, validation acc: 0.5660\n",
      "n_p:  0.287094292097\n",
      "Epoch: 107, train loss: 0.0681, train acc: 0.9812, validation acc: 0.5585\n",
      "n_p:  0.285473812963\n",
      "Epoch: 108, train loss: 0.0582, train acc: 0.9844, validation acc: 0.5623\n",
      "n_p:  0.283874516631\n",
      "Epoch: 109, train loss: 0.0732, train acc: 0.9762, validation acc: 0.5560\n",
      "n_p:  0.282295971197\n",
      "Epoch: 110, train loss: 0.0525, train acc: 0.9837, validation acc: 0.5409\n",
      "n_p:  0.280737756679\n",
      "Epoch: 111, train loss: 0.0577, train acc: 0.9815, validation acc: 0.5597\n",
      "n_p:  0.279199464599\n",
      "Epoch: 112, train loss: 0.0611, train acc: 0.9830, validation acc: 0.5623\n",
      "n_p:  0.277680697592\n",
      "Epoch: 113, train loss: 0.0516, train acc: 0.9837, validation acc: 0.5509\n",
      "n_p:  0.27618106902\n",
      "Epoch: 114, train loss: 0.0587, train acc: 0.9858, validation acc: 0.5673\n",
      "n_p:  0.274700202612\n",
      "Epoch: 115, train loss: 0.0524, train acc: 0.9851, validation acc: 0.5736\n",
      "n_p:  0.273237732114\n",
      "Epoch: 116, train loss: 0.0610, train acc: 0.9794, validation acc: 0.5623\n",
      "n_p:  0.271793300951\n",
      "Epoch: 117, train loss: 0.0440, train acc: 0.9886, validation acc: 0.5484\n",
      "n_p:  0.270366561908\n",
      "Epoch: 118, train loss: 0.0502, train acc: 0.9833, validation acc: 0.5560\n",
      "n_p:  0.26895717682\n",
      "Epoch: 119, train loss: 0.0564, train acc: 0.9847, validation acc: 0.5522\n",
      "n_p:  0.267564816275\n",
      "Epoch: 120, train loss: 0.0448, train acc: 0.9879, validation acc: 0.5648\n",
      "n_p:  0.26618915933\n",
      "Epoch: 121, train loss: 0.0578, train acc: 0.9819, validation acc: 0.5686\n",
      "n_p:  0.264829893234\n",
      "Epoch: 122, train loss: 0.0502, train acc: 0.9854, validation acc: 0.5535\n",
      "n_p:  0.263486713171\n",
      "Epoch: 123, train loss: 0.0515, train acc: 0.9854, validation acc: 0.5547\n",
      "n_p:  0.262159321998\n",
      "Epoch: 124, train loss: 0.0576, train acc: 0.9830, validation acc: 0.5434\n",
      "n_p:  0.260847430012\n",
      "Epoch: 125, train loss: 0.0530, train acc: 0.9837, validation acc: 0.5635\n",
      "n_p:  0.259550754707\n",
      "Epoch: 126, train loss: 0.0482, train acc: 0.9865, validation acc: 0.5547\n",
      "n_p:  0.258269020554\n",
      "Epoch: 127, train loss: 0.0400, train acc: 0.9883, validation acc: 0.5635\n",
      "n_p:  0.257001958783\n",
      "Epoch: 128, train loss: 0.0623, train acc: 0.9798, validation acc: 0.5635\n",
      "n_p:  0.255749307172\n",
      "Epoch: 129, train loss: 0.0469, train acc: 0.9858, validation acc: 0.5723\n",
      "n_p:  0.254510809851\n",
      "Epoch: 130, train loss: 0.0550, train acc: 0.9858, validation acc: 0.5761\n",
      "n_p:  0.253286217103\n",
      "Epoch: 131, train loss: 0.0442, train acc: 0.9865, validation acc: 0.5736\n",
      "n_p:  0.252075285183\n",
      "Epoch: 132, train loss: 0.0864, train acc: 0.9876, validation acc: 0.5472\n",
      "n_p:  0.250877776135\n",
      "Epoch: 133, train loss: 0.1354, train acc: 0.9631, validation acc: 0.5509\n",
      "n_p:  0.249693457617\n",
      "Epoch: 134, train loss: 0.0700, train acc: 0.9801, validation acc: 0.5648\n",
      "n_p:  0.24852210274\n",
      "Epoch: 135, train loss: 0.0726, train acc: 0.9847, validation acc: 0.5547\n",
      "n_p:  0.247363489902\n",
      "Epoch: 136, train loss: 0.0719, train acc: 0.9759, validation acc: 0.5572\n",
      "n_p:  0.246217402636\n",
      "Epoch: 137, train loss: 0.0623, train acc: 0.9773, validation acc: 0.5610\n",
      "n_p:  0.245083629458\n",
      "Epoch: 138, train loss: 0.0714, train acc: 0.9830, validation acc: 0.5748\n",
      "n_p:  0.243961963724\n",
      "Epoch: 139, train loss: 0.0687, train acc: 0.9819, validation acc: 0.5409\n",
      "n_p:  0.242852203489\n",
      "Epoch: 140, train loss: 0.0647, train acc: 0.9801, validation acc: 0.5321\n",
      "n_p:  0.241754151374\n",
      "Epoch: 141, train loss: 0.0528, train acc: 0.9826, validation acc: 0.5547\n",
      "n_p:  0.240667614437\n",
      "Epoch: 142, train loss: 0.0600, train acc: 0.9823, validation acc: 0.5547\n",
      "n_p:  0.239592404043\n",
      "Epoch: 143, train loss: 0.0490, train acc: 0.9840, validation acc: 0.5572\n",
      "n_p:  0.238528335748\n",
      "Epoch: 144, train loss: 0.0524, train acc: 0.9833, validation acc: 0.5547\n",
      "n_p:  0.23747522918\n",
      "Epoch: 145, train loss: 0.0450, train acc: 0.9865, validation acc: 0.5434\n",
      "n_p:  0.236432907923\n",
      "Epoch: 146, train loss: 0.0498, train acc: 0.9851, validation acc: 0.5610\n",
      "n_p:  0.235401199412\n",
      "Epoch: 147, train loss: 0.0580, train acc: 0.9815, validation acc: 0.5660\n",
      "n_p:  0.234379934826\n",
      "Epoch: 148, train loss: 0.0495, train acc: 0.9872, validation acc: 0.5686\n",
      "n_p:  0.233368948982\n",
      "Epoch: 149, train loss: 0.0560, train acc: 0.9837, validation acc: 0.5509\n",
      "n_p:  0.232368080243\n",
      "Epoch: 150, train loss: 0.0540, train acc: 0.9844, validation acc: 0.5497\n",
      "n_p:  0.231377170414\n",
      "Epoch: 151, train loss: 0.0437, train acc: 0.9862, validation acc: 0.5560\n",
      "n_p:  0.230396064659\n",
      "Epoch: 152, train loss: 0.0544, train acc: 0.9826, validation acc: 0.5623\n",
      "n_p:  0.229424611402\n",
      "Epoch: 153, train loss: 0.0408, train acc: 0.9876, validation acc: 0.5736\n",
      "n_p:  0.228462662248\n",
      "Epoch: 154, train loss: 0.0418, train acc: 0.9890, validation acc: 0.5660\n",
      "n_p:  0.227510071892\n",
      "Epoch: 155, train loss: 0.0482, train acc: 0.9837, validation acc: 0.5623\n",
      "n_p:  0.226566698046\n",
      "Epoch: 156, train loss: 0.0422, train acc: 0.9862, validation acc: 0.5484\n",
      "n_p:  0.225632401352\n",
      "Epoch: 157, train loss: 0.0523, train acc: 0.9844, validation acc: 0.5686\n",
      "n_p:  0.224707045312\n",
      "Epoch: 158, train loss: 0.0478, train acc: 0.9851, validation acc: 0.5686\n",
      "n_p:  0.22379049621\n",
      "Epoch: 159, train loss: 0.0361, train acc: 0.9894, validation acc: 0.5459\n",
      "n_p:  0.222882623044\n",
      "Epoch: 160, train loss: 0.0474, train acc: 0.9844, validation acc: 0.5711\n",
      "n_p:  0.221983297454\n",
      "Epoch: 161, train loss: 0.0410, train acc: 0.9865, validation acc: 0.5484\n",
      "n_p:  0.221092393656\n",
      "Epoch: 162, train loss: 0.0386, train acc: 0.9883, validation acc: 0.5698\n",
      "n_p:  0.220209788377\n",
      "Epoch: 163, train loss: 0.0392, train acc: 0.9894, validation acc: 0.5610\n",
      "n_p:  0.21933536079\n",
      "Epoch: 164, train loss: 0.0367, train acc: 0.9922, validation acc: 0.5660\n",
      "n_p:  0.218468992457\n",
      "Epoch: 165, train loss: 0.0420, train acc: 0.9858, validation acc: 0.5522\n",
      "n_p:  0.217610567264\n",
      "Epoch: 166, train loss: 0.0350, train acc: 0.9915, validation acc: 0.5723\n",
      "n_p:  0.216759971369\n",
      "Epoch: 167, train loss: 0.0390, train acc: 0.9894, validation acc: 0.5623\n",
      "n_p:  0.215917093141\n",
      "Epoch: 168, train loss: 0.0411, train acc: 0.9890, validation acc: 0.5572\n",
      "n_p:  0.215081823111\n",
      "Epoch: 169, train loss: 0.0430, train acc: 0.9869, validation acc: 0.5673\n",
      "n_p:  0.214254053912\n",
      "Epoch: 170, train loss: 0.0357, train acc: 0.9904, validation acc: 0.5660\n",
      "n_p:  0.213433680238\n",
      "Epoch: 171, train loss: 0.0338, train acc: 0.9883, validation acc: 0.5761\n",
      "n_p:  0.212620598785\n",
      "Epoch: 172, train loss: 0.0450, train acc: 0.9854, validation acc: 0.5874\n",
      "n_p:  0.211814708209\n",
      "Epoch: 173, train loss: 0.0337, train acc: 0.9897, validation acc: 0.5623\n",
      "n_p:  0.211015909074\n",
      "Epoch: 174, train loss: 0.0400, train acc: 0.9876, validation acc: 0.5560\n",
      "n_p:  0.210224103813\n",
      "Epoch: 175, train loss: 0.0385, train acc: 0.9872, validation acc: 0.5698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_p:  0.209439196679\n",
      "Epoch: 176, train loss: 0.0452, train acc: 0.9883, validation acc: 0.5673\n",
      "n_p:  0.208661093703\n",
      "Epoch: 177, train loss: 0.0394, train acc: 0.9869, validation acc: 0.5623\n",
      "n_p:  0.207889702654\n",
      "Epoch: 178, train loss: 0.0368, train acc: 0.9922, validation acc: 0.5597\n",
      "n_p:  0.207124932996\n",
      "Epoch: 179, train loss: 0.0502, train acc: 0.9830, validation acc: 0.5547\n",
      "n_p:  0.206366695852\n",
      "Epoch: 180, train loss: 0.0405, train acc: 0.9876, validation acc: 0.5635\n",
      "n_p:  0.205614903962\n",
      "Epoch: 181, train loss: 0.0477, train acc: 0.9865, validation acc: 0.5623\n",
      "n_p:  0.20486947165\n",
      "Epoch: 182, train loss: 0.0355, train acc: 0.9901, validation acc: 0.5560\n",
      "n_p:  0.204130314783\n",
      "Epoch: 183, train loss: 0.0439, train acc: 0.9876, validation acc: 0.5358\n",
      "n_p:  0.203397350741\n",
      "Epoch: 184, train loss: 0.0404, train acc: 0.9865, validation acc: 0.5522\n",
      "n_p:  0.202670498379\n",
      "Epoch: 185, train loss: 0.0712, train acc: 0.9876, validation acc: 0.5786\n",
      "n_p:  0.201949677994\n",
      "Epoch: 186, train loss: 0.0889, train acc: 0.9780, validation acc: 0.5547\n",
      "n_p:  0.201234811296\n",
      "Epoch: 187, train loss: 0.0421, train acc: 0.9879, validation acc: 0.5547\n",
      "n_p:  0.200525821372\n",
      "Epoch: 188, train loss: 0.0491, train acc: 0.9851, validation acc: 0.5522\n",
      "n_p:  0.199822632658\n",
      "Epoch: 189, train loss: 0.0339, train acc: 0.9918, validation acc: 0.5660\n",
      "n_p:  0.199125170911\n",
      "Epoch: 190, train loss: 0.0439, train acc: 0.9865, validation acc: 0.5761\n",
      "n_p:  0.198433363174\n",
      "Epoch: 191, train loss: 0.0416, train acc: 0.9883, validation acc: 0.5597\n",
      "n_p:  0.197747137754\n",
      "Epoch: 192, train loss: 0.0419, train acc: 0.9872, validation acc: 0.5774\n",
      "n_p:  0.197066424191\n",
      "Epoch: 193, train loss: 0.0417, train acc: 0.9890, validation acc: 0.5686\n",
      "n_p:  0.196391153234\n",
      "Epoch: 194, train loss: 0.0417, train acc: 0.9886, validation acc: 0.5472\n",
      "n_p:  0.19572125681\n",
      "Epoch: 195, train loss: 0.0386, train acc: 0.9865, validation acc: 0.5597\n",
      "n_p:  0.195056668004\n",
      "Epoch: 196, train loss: 0.0343, train acc: 0.9901, validation acc: 0.5748\n",
      "n_p:  0.194397321031\n",
      "Epoch: 197, train loss: 0.0404, train acc: 0.9876, validation acc: 0.5648\n",
      "n_p:  0.193743151214\n",
      "Epoch: 198, train loss: 0.0378, train acc: 0.9901, validation acc: 0.5447\n",
      "n_p:  0.193094094959\n",
      "Epoch: 199, train loss: 0.0386, train acc: 0.9897, validation acc: 0.5497\n",
      "n_p:  0.19245008973\n",
      "Epoch: 200, train loss: 0.0363, train acc: 0.9894, validation acc: 0.5736\n",
      "n_p:  0.191811074033\n",
      "Epoch: 201, train loss: 0.0364, train acc: 0.9883, validation acc: 0.5799\n",
      "n_p:  0.191176987388\n",
      "Epoch: 202, train loss: 0.0535, train acc: 0.9833, validation acc: 0.5799\n",
      "n_p:  0.190547770311\n",
      "Epoch: 203, train loss: 0.0342, train acc: 0.9915, validation acc: 0.5736\n",
      "n_p:  0.189923364293\n",
      "Epoch: 204, train loss: 0.0420, train acc: 0.9858, validation acc: 0.5660\n",
      "n_p:  0.189303711779\n",
      "Epoch: 205, train loss: 0.0392, train acc: 0.9879, validation acc: 0.5761\n",
      "n_p:  0.188688756149\n",
      "Epoch: 206, train loss: 0.0369, train acc: 0.9876, validation acc: 0.5660\n",
      "n_p:  0.188078441698\n",
      "Epoch: 207, train loss: 0.0356, train acc: 0.9904, validation acc: 0.5786\n",
      "n_p:  0.18747271362\n",
      "Epoch: 208, train loss: 0.0325, train acc: 0.9915, validation acc: 0.5509\n",
      "n_p:  0.186871517985\n",
      "Epoch: 209, train loss: 0.0433, train acc: 0.9876, validation acc: 0.5836\n",
      "n_p:  0.186274801726\n",
      "Epoch: 210, train loss: 0.0476, train acc: 0.9854, validation acc: 0.5547\n",
      "n_p:  0.18568251262\n",
      "Epoch: 211, train loss: 0.0373, train acc: 0.9897, validation acc: 0.5572\n",
      "n_p:  0.185094599268\n",
      "Epoch: 212, train loss: 0.0314, train acc: 0.9915, validation acc: 0.5484\n",
      "n_p:  0.184511011085\n",
      "Epoch: 213, train loss: 0.0324, train acc: 0.9925, validation acc: 0.5635\n",
      "n_p:  0.183931698278\n",
      "Epoch: 214, train loss: 0.0386, train acc: 0.9879, validation acc: 0.5673\n",
      "n_p:  0.183356611832\n",
      "Epoch: 215, train loss: 0.0372, train acc: 0.9897, validation acc: 0.5610\n",
      "n_p:  0.182785703496\n",
      "Epoch: 216, train loss: 0.0372, train acc: 0.9876, validation acc: 0.5660\n",
      "n_p:  0.182218925767\n",
      "Epoch: 217, train loss: 0.0347, train acc: 0.9894, validation acc: 0.5472\n",
      "n_p:  0.181656231875\n",
      "Epoch: 218, train loss: 0.0314, train acc: 0.9925, validation acc: 0.5723\n",
      "n_p:  0.181097575771\n",
      "Epoch: 219, train loss: 0.0382, train acc: 0.9876, validation acc: 0.5509\n",
      "n_p:  0.180542912107\n",
      "Epoch: 220, train loss: 0.0382, train acc: 0.9886, validation acc: 0.5497\n",
      "n_p:  0.179992196232\n",
      "Epoch: 221, train loss: 0.0409, train acc: 0.9872, validation acc: 0.5736\n",
      "n_p:  0.179445384169\n",
      "Epoch: 222, train loss: 0.0403, train acc: 0.9890, validation acc: 0.5698\n",
      "n_p:  0.178902432608\n",
      "Epoch: 223, train loss: 0.0349, train acc: 0.9904, validation acc: 0.5711\n",
      "n_p:  0.178363298892\n",
      "Epoch: 224, train loss: 0.0353, train acc: 0.9908, validation acc: 0.5623\n",
      "n_p:  0.177827941004\n",
      "Epoch: 225, train loss: 0.0328, train acc: 0.9897, validation acc: 0.5535\n",
      "n_p:  0.177296317553\n",
      "Epoch: 226, train loss: 0.0334, train acc: 0.9901, validation acc: 0.5635\n",
      "n_p:  0.176768387768\n",
      "Epoch: 227, train loss: 0.0289, train acc: 0.9908, validation acc: 0.5535\n",
      "n_p:  0.176244111479\n",
      "Epoch: 228, train loss: 0.0327, train acc: 0.9901, validation acc: 0.5547\n",
      "n_p:  0.175723449112\n",
      "Epoch: 229, train loss: 0.0378, train acc: 0.9890, validation acc: 0.5547\n",
      "n_p:  0.175206361673\n",
      "Epoch: 230, train loss: 0.0288, train acc: 0.9925, validation acc: 0.5660\n",
      "n_p:  0.174692810742\n",
      "Epoch: 231, train loss: 0.0412, train acc: 0.9869, validation acc: 0.5799\n",
      "n_p:  0.174182758458\n",
      "Epoch: 232, train loss: 0.0290, train acc: 0.9915, validation acc: 0.5522\n",
      "n_p:  0.17367616751\n",
      "Epoch: 233, train loss: 0.0308, train acc: 0.9897, validation acc: 0.5610\n",
      "n_p:  0.17317300113\n",
      "Epoch: 234, train loss: 0.0342, train acc: 0.9886, validation acc: 0.5711\n",
      "n_p:  0.172673223079\n",
      "Epoch: 235, train loss: 0.0342, train acc: 0.9886, validation acc: 0.5723\n",
      "n_p:  0.172176797639\n",
      "Epoch: 236, train loss: 0.0285, train acc: 0.9925, validation acc: 0.5761\n",
      "n_p:  0.171683689603\n",
      "Epoch: 237, train loss: 0.0261, train acc: 0.9933, validation acc: 0.5660\n",
      "n_p:  0.171193864266\n",
      "Epoch: 238, train loss: 0.0311, train acc: 0.9904, validation acc: 0.5497\n",
      "n_p:  0.170707287419\n",
      "Epoch: 239, train loss: 0.0326, train acc: 0.9915, validation acc: 0.5736\n",
      "n_p:  0.170223925335\n",
      "Epoch: 240, train loss: 0.0317, train acc: 0.9904, validation acc: 0.5711\n",
      "n_p:  0.169743744761\n",
      "Epoch: 241, train loss: 0.0408, train acc: 0.9883, validation acc: 0.5711\n",
      "n_p:  0.169266712916\n",
      "Epoch: 242, train loss: 0.0315, train acc: 0.9922, validation acc: 0.5522\n",
      "n_p:  0.168792797473\n",
      "Epoch: 243, train loss: 0.0350, train acc: 0.9908, validation acc: 0.5610\n",
      "n_p:  0.168321966558\n",
      "Epoch: 244, train loss: 0.0294, train acc: 0.9911, validation acc: 0.5648\n",
      "n_p:  0.167854188742\n",
      "Epoch: 245, train loss: 0.0376, train acc: 0.9894, validation acc: 0.5799\n",
      "n_p:  0.167389433027\n",
      "Epoch: 246, train loss: 0.0297, train acc: 0.9908, validation acc: 0.5862\n",
      "n_p:  0.166927668846\n",
      "Epoch: 247, train loss: 0.0300, train acc: 0.9904, validation acc: 0.5660\n",
      "n_p:  0.16646886605\n",
      "Epoch: 248, train loss: 0.0349, train acc: 0.9897, validation acc: 0.5774\n",
      "n_p:  0.166012994903\n",
      "Epoch: 249, train loss: 0.0260, train acc: 0.9929, validation acc: 0.5509\n",
      "n_p:  0.165560026076\n",
      "Epoch: 250, train loss: 0.0360, train acc: 0.9915, validation acc: 0.5723\n"
     ]
    }
   ],
   "source": [
    "p = np.linspace(float(1./opt['p_lr_decay']),1,opt['p_lr_decay'])\n",
    "\n",
    "for epoch in range(opt['epochs']):\n",
    "    srcData = iter(src_dataloader)\n",
    "    totalCorrects = 0.\n",
    "    totalClsLoss = 0.\n",
    "#     experiment.log_current_epoch(epoch)\n",
    "    \n",
    "    n_p = opt['n0'] / pow((1. + opt['alpha'] * p[epoch]), (opt['beta']))\n",
    "    print(\"n_p: \", n_p)\n",
    "    for param_group in optimizer_2.param_groups:\n",
    "        param_group['lr'] = opt['lr'] * param_group['lr_mul'] * n_p\n",
    "        \n",
    "    for srcimgs, srclbls in srcData:\n",
    "        srcimgs = srcimgs.cuda() if opt['use_gpu'] else srcimgs\n",
    "        srclbls = srclbls.cuda() if opt['use_gpu'] else srclbls\n",
    "        srcimgs, srclbls = Variable(srcimgs), Variable(srclbls)\n",
    "        \n",
    "        feat_ = net(srcimgs)\n",
    "        logits = net2(feat_)\n",
    "        \n",
    "        clsloss = criterion(logits, srclbls)\n",
    "        totalClsLoss += clsloss.data[0] * opt['batchSize']\n",
    "        \n",
    "        _, preds = torch.max(softmax(logits).data, 1)\n",
    "        totalCorrects += torch.eq(preds, srclbls.data).float().sum()\n",
    "               \n",
    "        optimizer_2.zero_grad()\n",
    "        clsloss.backward()\n",
    "        optimizer_2.step()\n",
    "        \n",
    "        if lr_sch: lr_sch.step()\n",
    "        \n",
    "    srcAcc = totalCorrects / opt['srcDataLen']\n",
    "    srcLoss = totalClsLoss / opt['srcDataLen']\n",
    "    valAcc = get_validation_acc()\n",
    "#     experiment.log_metric(\"src_clsLoss\", srcLoss, step=epoch)\n",
    "#     experiment.log_metric(\"src_clsAcc\", srcAcc, step=epoch)\n",
    "#     experiment.log_metric('val_accuracy', valAcc, step=epoch)\n",
    "#     experiment.log_epoch_end(epoch)\n",
    "    print(\"Epoch: {}, train loss: {:.4f}, train acc: {:.4f}, validation acc: {:.4f}\".\n",
    "          format(epoch+1, srcLoss, srcAcc, valAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "valAcc, y_preds, y_true = get_validation_acc(cm=True)\n",
    "\n",
    "y_preds = [d for sublist in y_preds for d in sublist]\n",
    "y_true = [d for sublist in y_true for d in sublist]\n",
    "\n",
    "cm = confusion_matrix(y_true, y_preds)\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure(figsize=(20,20))\n",
    "utils.plot_confusion_matrix(cm, classes=src_dataset.classes, normalize=False)\n",
    "plt.show()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
